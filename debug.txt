Traceback (most recent call last):
  File "/mnt/berry/home/yihang/pyenv/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq_cli/train.py", line 354, in cli_main
    distributed_utils.call_main(args, main)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/distributed_utils.py", line 275, in call_main
    infer_init_method(args)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/distributed_utils.py", line 146, in infer_init_method
    assert args.distributed_world_size <= torch.cuda.device_count()
AssertionError
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 6): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 4): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 7): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 5): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2022-06-30 13:30:57 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:18957
2022-06-30 13:30:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-06-30 13:30:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2022-06-30 13:30:57 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 0
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 2
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 6
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 1
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 4
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 3
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 7
2022-06-30 13:30:58 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 13:30:58 | INFO | fairseq.distributed_utils | initialized host kubera as rank 5
2022-06-30 13:31:06 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, checkpoint_suffix='', checkpoint_shard_count=1, quantization_config_path=None, profile=False, criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='image_mmt', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=1000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=1000, batch_size_valid=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, distributed_world_size=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:18957', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, fast_stat_sync=False, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', arch='image_multimodal_transformer_SA_top', max_epoch=0, max_update=8000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.005], min_lr=1e-09, use_bmuf=False, save_dir='checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=10, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, SA_image_dropout=0.1, SA_text_dropout=0, SA_attention_dropout=0.1, image_pre_norm=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, warmup_updates=2000, warmup_init_lr=1e-07, data='data-bin/multi30k.en-de', source_lang='en', target_lang='de', load_alignments=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, image_feat_path=['data/vit_base_patch16_384'], image_feat_dim=[768], dropout=0.3, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=128, encoder_ffn_embed_dim=256, encoder_attention_heads=4, encoder_layers=4, decoder_embed_dim=128, decoder_ffn_embed_dim=256, decoder_attention_heads=4, decoder_layers=4, is_fusion_top=True, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=128, decoder_input_dim=128, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, distributed_num_procs=8)
2022-06-30 13:31:06 | INFO | fairseq.tasks.image_multimodal_translation | [en] dictionary: 9712 types
2022-06-30 13:31:06 | INFO | fairseq.tasks.image_multimodal_translation | [de] dictionary: 9712 types
2022-06-30 13:31:06 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.en
2022-06-30 13:31:06 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.de
2022-06-30 13:31:06 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de valid en-de 1014 examples
2022-06-30 13:31:08 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9712, 128, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (selective_attns): ModuleList(
      (0): SelectiveAttention(
        (q_proj): Linear(in_features=128, out_features=128, bias=True)
        (k_proj): Linear(in_features=768, out_features=128, bias=True)
        (v_proj): Linear(in_features=768, out_features=128, bias=True)
        (attn_drop): Dropout(p=0.1, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (gate_denses): ModuleList(
      (0): Linear(in_features=256, out_features=128, bias=True)
    )
    (image_dropout_module): FairseqDropout()
    (text_dropout_module): FairseqDropout()
    (image_pre_norm_module): Identity()
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9712, 128, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=128, out_features=9712, bias=False)
  )
)
2022-06-30 13:31:08 | INFO | fairseq_cli.train | task: image_mmt (ImageMMTTask)
2022-06-30 13:31:08 | INFO | fairseq_cli.train | model: image_multimodal_transformer_SA_top (TransformerModel)
2022-06-30 13:31:08 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2022-06-30 13:31:08 | INFO | fairseq_cli.train | num. model params: 2830976 (num. trained: 2830976)
2022-06-30 13:31:08 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-06-30 13:31:08 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-06-30 13:31:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   4: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   5: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   6: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | rank   7: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 13:31:08 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2022-06-30 13:31:08 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2022-06-30 13:31:08 | INFO | fairseq_cli.train | max tokens per GPU = 1000 and max sentences per GPU = None
2022-06-30 13:31:08 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint_last.pt
2022-06-30 13:31:08 | INFO | fairseq.trainer | loading train data for epoch 1
2022-06-30 13:31:08 | INFO | fairseq.data.data_utils | loaded 29000 examples from: data-bin/multi30k.en-de/train.en-de.en
2022-06-30 13:31:08 | INFO | fairseq.data.data_utils | loaded 29000 examples from: data-bin/multi30k.en-de/train.en-de.de
2022-06-30 13:31:08 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de train en-de 29000 examples
usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]
                     [--log-format LOG_FORMAT]
                     [--tensorboard-logdir TENSORBOARD_LOGDIR] [--seed SEED]
                     [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16]
                     [--fp16] [--memory-efficient-fp16]
                     [--fp16-no-flatten-grads]
                     [--fp16-init-scale FP16_INIT_SCALE]
                     [--fp16-scale-window FP16_SCALE_WINDOW]
                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
                     [--min-loss-scale MIN_LOSS_SCALE]
                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE]
                     [--user-dir USER_DIR]
                     [--empty-cache-freq EMPTY_CACHE_FREQ]
                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]
                     [--model-parallel-size MODEL_PARALLEL_SIZE]
                     [--checkpoint-suffix CHECKPOINT_SUFFIX]
                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]
                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]
                     [--profile]
                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,legacy_masked_lm_loss,masked_lm,nat_loss,sentence_prediction,sentence_ranking,wav2vec,vocab_parallel_cross_entropy}]
                     [--tokenizer {moses,nltk,space}]
                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]
                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,lamb,nag,sgd}]
                     [--lr-scheduler {cosine,fixed,inverse_sqrt,polynomial_decay,reduce_lr_on_plateau,tri_stage,triangular}]
                     [--scoring {sacrebleu,bleu,chrf,wer}] [--task TASK]
                     [--num-workers NUM_WORKERS]
                     [--skip-invalid-size-inputs-valid-test]
                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]
                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]
                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]
                     [--dataset-impl DATASET_IMPL]
                     [--data-buffer-size DATA_BUFFER_SIZE]
                     [--train-subset TRAIN_SUBSET]
                     [--valid-subset VALID_SUBSET]
                     [--validate-interval VALIDATE_INTERVAL]
                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]
                     [--validate-after-updates VALIDATE_AFTER_UPDATES]
                     [--fixed-validation-seed FIXED_VALIDATION_SEED]
                     [--disable-validation]
                     [--max-tokens-valid MAX_TOKENS_VALID]
                     [--batch-size-valid BATCH_SIZE_VALID]
                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]
                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]
                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]
                     [--distributed-rank DISTRIBUTED_RANK]
                     [--distributed-backend DISTRIBUTED_BACKEND]
                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]
                     [--distributed-port DISTRIBUTED_PORT]
                     [--device-id DEVICE_ID] [--distributed-no-spawn]
                     [--ddp-backend {c10d,no_c10d}]
                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]
                     [--find-unused-parameters] [--fast-stat-sync]
                     [--broadcast-buffers]
                     [--distributed-wrapper {DDP,SlowMo}]
                     [--slowmo-momentum SLOWMO_MOMENTUM]
                     [--slowmo-algorithm SLOWMO_ALGORITHM]
                     [--localsgd-frequency LOCALSGD_FREQUENCY]
                     [--nprocs-per-node NPROCS_PER_NODE]
                     [--pipeline-model-parallel]
                     [--pipeline-balance PIPELINE_BALANCE]
                     [--pipeline-devices PIPELINE_DEVICES]
                     [--pipeline-chunks PIPELINE_CHUNKS]
                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]
                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]
                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]
                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]
                     [--pipeline-checkpoint {always,never,except_last}]
                     [--zero-sharding {none,os}] [--arch ARCH]
                     [--max-epoch MAX_EPOCH] [--max-update MAX_UPDATE]
                     [--stop-time-hours STOP_TIME_HOURS]
                     [--clip-norm CLIP_NORM] [--sentence-avg]
                     [--update-freq UPDATE_FREQ] [--lr LR] [--min-lr MIN_LR]
                     [--use-bmuf] [--save-dir SAVE_DIR]
                     [--restore-file RESTORE_FILE]
                     [--finetune-from-model FINETUNE_FROM_MODEL]
                     [--reset-dataloader] [--reset-lr-scheduler]
                     [--reset-meters] [--reset-optimizer]
                     [--optimizer-overrides OPTIMIZER_OVERRIDES]
                     [--save-interval SAVE_INTERVAL]
                     [--save-interval-updates SAVE_INTERVAL_UPDATES]
                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]
                     [--keep-last-epochs KEEP_LAST_EPOCHS]
                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]
                     [--no-save] [--no-epoch-checkpoints]
                     [--no-last-checkpoints] [--no-save-optimizer-state]
                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]
                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]
                     [--activation-fn {relu,gelu,gelu_fast,gelu_accurate,tanh,linear}]
                     [--dropout D] [--attention-dropout D]
                     [--activation-dropout D] [--encoder-embed-path STR]
                     [--encoder-embed-dim N] [--encoder-ffn-embed-dim N]
                     [--encoder-layers N] [--encoder-attention-heads N]
                     [--encoder-normalize-before] [--encoder-learned-pos]
                     [--decoder-embed-path STR] [--decoder-embed-dim N]
                     [--decoder-ffn-embed-dim N] [--decoder-layers N]
                     [--decoder-attention-heads N] [--decoder-learned-pos]
                     [--decoder-normalize-before] [--decoder-output-dim N]
                     [--share-decoder-input-output-embed]
                     [--share-all-embeddings]
                     [--no-token-positional-embeddings]
                     [--adaptive-softmax-cutoff EXPR]
                     [--adaptive-softmax-dropout D] [--layernorm-embedding]
                     [--no-scale-embedding] [--no-cross-attention]
                     [--cross-self-attention] [--encoder-layerdrop D]
                     [--decoder-layerdrop D]
                     [--encoder-layers-to-keep ENCODER_LAYERS_TO_KEEP]
                     [--decoder-layers-to-keep DECODER_LAYERS_TO_KEEP]
                     [--quant-noise-pq D] [--quant-noise-pq-block-size D]
                     [--quant-noise-scalar D]
                     [--SA-image-dropout SA_IMAGE_DROPOUT]
                     [--SA-text-dropout SA_TEXT_DROPOUT]
                     [--SA-attention-dropout SA_ATTENTION_DROPOUT]
                     [--image-pre-norm] [--is-fusion-top IS_FUSION_TOP]
                     [--label-smoothing D] [--report-accuracy]
                     [--ignore-prefix-size IGNORE_PREFIX_SIZE]
                     [--adam-betas ADAM_BETAS] [--adam-eps ADAM_EPS]
                     [--weight-decay WEIGHT_DECAY] [--use-old-adam]
                     [--warmup-updates WARMUP_UPDATES]
                     [--warmup-init-lr WARMUP_INIT_LR] [-s SRC] [-t TARGET]
                     [--load-alignments] [--left-pad-source BOOL]
                     [--left-pad-target BOOL] [--max-source-positions N]
                     [--max-target-positions N]
                     [--upsample-primary UPSAMPLE_PRIMARY] [--truncate-source]
                     [--num-batch-buckets N] [--eval-bleu]
                     [--eval-bleu-detok EVAL_BLEU_DETOK]
                     [--eval-bleu-detok-args JSON] [--eval-tokenized-bleu]
                     [--eval-bleu-remove-bpe [EVAL_BLEU_REMOVE_BPE]]
                     [--eval-bleu-args JSON] [--eval-bleu-print-samples]
                     [--image-feat-path IMAGE_FEAT_PATH [IMAGE_FEAT_PATH ...]]
                     [--image-feat-dim IMAGE_FEAT_DIM [IMAGE_FEAT_DIM ...]]
                     data
fairseq-train: error: unrecognized arguments: --lazy-load
usage: fairseq-train [-h] [--no-progress-bar] [--log-interval LOG_INTERVAL]
                     [--log-format LOG_FORMAT]
                     [--tensorboard-logdir TENSORBOARD_LOGDIR] [--seed SEED]
                     [--cpu] [--tpu] [--bf16] [--memory-efficient-bf16]
                     [--fp16] [--memory-efficient-fp16]
                     [--fp16-no-flatten-grads]
                     [--fp16-init-scale FP16_INIT_SCALE]
                     [--fp16-scale-window FP16_SCALE_WINDOW]
                     [--fp16-scale-tolerance FP16_SCALE_TOLERANCE]
                     [--min-loss-scale MIN_LOSS_SCALE]
                     [--threshold-loss-scale THRESHOLD_LOSS_SCALE]
                     [--user-dir USER_DIR]
                     [--empty-cache-freq EMPTY_CACHE_FREQ]
                     [--all-gather-list-size ALL_GATHER_LIST_SIZE]
                     [--model-parallel-size MODEL_PARALLEL_SIZE]
                     [--checkpoint-suffix CHECKPOINT_SUFFIX]
                     [--checkpoint-shard-count CHECKPOINT_SHARD_COUNT]
                     [--quantization-config-path QUANTIZATION_CONFIG_PATH]
                     [--profile]
                     [--criterion {adaptive_loss,composite_loss,cross_entropy,ctc,label_smoothed_cross_entropy,label_smoothed_cross_entropy_with_alignment,legacy_masked_lm_loss,masked_lm,nat_loss,sentence_prediction,sentence_ranking,wav2vec,vocab_parallel_cross_entropy}]
                     [--tokenizer {moses,nltk,space}]
                     [--bpe {byte_bpe,bytes,characters,fastbpe,gpt2,bert,hf_byte_bpe,sentencepiece,subword_nmt}]
                     [--optimizer {adadelta,adafactor,adagrad,adam,adamax,lamb,nag,sgd}]
                     [--lr-scheduler {cosine,fixed,inverse_sqrt,polynomial_decay,reduce_lr_on_plateau,tri_stage,triangular}]
                     [--scoring {sacrebleu,bleu,chrf,wer}] [--task TASK]
                     [--num-workers NUM_WORKERS]
                     [--skip-invalid-size-inputs-valid-test]
                     [--max-tokens MAX_TOKENS] [--batch-size BATCH_SIZE]
                     [--required-batch-size-multiple REQUIRED_BATCH_SIZE_MULTIPLE]
                     [--required-seq-len-multiple REQUIRED_SEQ_LEN_MULTIPLE]
                     [--dataset-impl DATASET_IMPL]
                     [--data-buffer-size DATA_BUFFER_SIZE]
                     [--train-subset TRAIN_SUBSET]
                     [--valid-subset VALID_SUBSET]
                     [--validate-interval VALIDATE_INTERVAL]
                     [--validate-interval-updates VALIDATE_INTERVAL_UPDATES]
                     [--validate-after-updates VALIDATE_AFTER_UPDATES]
                     [--fixed-validation-seed FIXED_VALIDATION_SEED]
                     [--disable-validation]
                     [--max-tokens-valid MAX_TOKENS_VALID]
                     [--batch-size-valid BATCH_SIZE_VALID]
                     [--curriculum CURRICULUM] [--gen-subset GEN_SUBSET]
                     [--num-shards NUM_SHARDS] [--shard-id SHARD_ID]
                     [--distributed-world-size DISTRIBUTED_WORLD_SIZE]
                     [--distributed-rank DISTRIBUTED_RANK]
                     [--distributed-backend DISTRIBUTED_BACKEND]
                     [--distributed-init-method DISTRIBUTED_INIT_METHOD]
                     [--distributed-port DISTRIBUTED_PORT]
                     [--device-id DEVICE_ID] [--distributed-no-spawn]
                     [--ddp-backend {c10d,no_c10d}]
                     [--bucket-cap-mb BUCKET_CAP_MB] [--fix-batches-to-gpus]
                     [--find-unused-parameters] [--fast-stat-sync]
                     [--broadcast-buffers]
                     [--distributed-wrapper {DDP,SlowMo}]
                     [--slowmo-momentum SLOWMO_MOMENTUM]
                     [--slowmo-algorithm SLOWMO_ALGORITHM]
                     [--localsgd-frequency LOCALSGD_FREQUENCY]
                     [--nprocs-per-node NPROCS_PER_NODE]
                     [--pipeline-model-parallel]
                     [--pipeline-balance PIPELINE_BALANCE]
                     [--pipeline-devices PIPELINE_DEVICES]
                     [--pipeline-chunks PIPELINE_CHUNKS]
                     [--pipeline-encoder-balance PIPELINE_ENCODER_BALANCE]
                     [--pipeline-encoder-devices PIPELINE_ENCODER_DEVICES]
                     [--pipeline-decoder-balance PIPELINE_DECODER_BALANCE]
                     [--pipeline-decoder-devices PIPELINE_DECODER_DEVICES]
                     [--pipeline-checkpoint {always,never,except_last}]
                     [--zero-sharding {none,os}] [--arch ARCH]
                     [--max-epoch MAX_EPOCH] [--max-update MAX_UPDATE]
                     [--stop-time-hours STOP_TIME_HOURS]
                     [--clip-norm CLIP_NORM] [--sentence-avg]
                     [--update-freq UPDATE_FREQ] [--lr LR] [--min-lr MIN_LR]
                     [--use-bmuf] [--save-dir SAVE_DIR]
                     [--restore-file RESTORE_FILE]
                     [--finetune-from-model FINETUNE_FROM_MODEL]
                     [--reset-dataloader] [--reset-lr-scheduler]
                     [--reset-meters] [--reset-optimizer]
                     [--optimizer-overrides OPTIMIZER_OVERRIDES]
                     [--save-interval SAVE_INTERVAL]
                     [--save-interval-updates SAVE_INTERVAL_UPDATES]
                     [--keep-interval-updates KEEP_INTERVAL_UPDATES]
                     [--keep-last-epochs KEEP_LAST_EPOCHS]
                     [--keep-best-checkpoints KEEP_BEST_CHECKPOINTS]
                     [--no-save] [--no-epoch-checkpoints]
                     [--no-last-checkpoints] [--no-save-optimizer-state]
                     [--best-checkpoint-metric BEST_CHECKPOINT_METRIC]
                     [--maximize-best-checkpoint-metric] [--patience PATIENCE]
fairseq-train: error: argument --dataset-impl: invalid typing.Optional[fairseq.dataclass.utils.Choices] value: 'lazy'
2022-06-30 17:22:54 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | fairseq.distributed_utils | distributed init (rank 6): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | fairseq.distributed_utils | distributed init (rank 4): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | fairseq.distributed_utils | distributed init (rank 5): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2022-06-30 17:22:55 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2022-06-30 17:22:55 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | fairseq.distributed_utils | distributed init (rank 7): tcp://localhost:16500
2022-06-30 17:22:55 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-06-30 17:22:55 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2022-06-30 17:22:55 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 0
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 1
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 2
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 4
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 7
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 5
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 3
2022-06-30 17:22:56 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:22:56 | INFO | fairseq.distributed_utils | initialized host kubera as rank 6
2022-06-30 17:23:03 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, checkpoint_suffix='', checkpoint_shard_count=1, quantization_config_path=None, profile=False, criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='image_mmt', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=1000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=1000, batch_size_valid=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, distributed_world_size=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:16500', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, fast_stat_sync=False, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', arch='image_multimodal_transformer_SA_top', max_epoch=0, max_update=8000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.005], min_lr=1e-09, use_bmuf=False, save_dir='checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=10, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, SA_image_dropout=0.1, SA_text_dropout=0, SA_attention_dropout=0.1, image_pre_norm=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, warmup_updates=2000, warmup_init_lr=1e-07, data='data-bin/multi30k.en-de', source_lang='en', target_lang='de', load_alignments=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, image_feat_path=['data/vit_base_patch16_384'], image_feat_dim=[768], dropout=0.3, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=128, encoder_ffn_embed_dim=256, encoder_attention_heads=4, encoder_layers=4, decoder_embed_dim=128, decoder_ffn_embed_dim=256, decoder_attention_heads=4, decoder_layers=4, is_fusion_top=True, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=128, decoder_input_dim=128, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, distributed_num_procs=8)
2022-06-30 17:23:03 | INFO | fairseq.tasks.image_multimodal_translation | [en] dictionary: 9712 types
2022-06-30 17:23:03 | INFO | fairseq.tasks.image_multimodal_translation | [de] dictionary: 9712 types
2022-06-30 17:23:03 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.en
2022-06-30 17:23:03 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.de
2022-06-30 17:23:03 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de valid en-de 1014 examples
Traceback (most recent call last):
  File "/mnt/berry/home/yihang/pyenv/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq_cli/train.py", line 354, in cli_main
    distributed_utils.call_main(args, main)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/distributed_utils.py", line 283, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 6 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/distributed_utils.py", line 270, in distributed_main
    main(args, **kwargs)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq_cli/train.py", line 65, in main
    task.load_dataset(valid_sub_split, combine=False, epoch=1)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/tasks/image_multimodal_translation.py", line 314, in load_dataset
    self.datasets[split] = load_langpair_dataset(
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/tasks/image_multimodal_translation.py", line 152, in load_langpair_dataset
    assert os.path.exists(feat_pth_path) == True, f'not found image feature {feat_pth_path}'
AssertionError: not found image feature data/vit_base_patch16_384/valid

2022-06-30 17:27:26 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | distributed init (rank 7): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | distributed init (rank 4): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | distributed init (rank 6): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | distributed init (rank 5): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15295
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 0
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 3
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 4
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 1
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 2
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 5
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 7
2022-06-30 17:27:27 | INFO | fairseq.distributed_utils | initialized host kubera as rank 6
2022-06-30 17:27:35 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, checkpoint_suffix='', checkpoint_shard_count=1, quantization_config_path=None, profile=False, criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='image_mmt', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=1000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=1000, batch_size_valid=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, distributed_world_size=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:15295', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, fast_stat_sync=False, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', arch='image_multimodal_transformer_SA_top', max_epoch=0, max_update=8000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.005], min_lr=1e-09, use_bmuf=False, save_dir='checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=10, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, SA_image_dropout=0.1, SA_text_dropout=0, SA_attention_dropout=0.1, image_pre_norm=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, warmup_updates=2000, warmup_init_lr=1e-07, data='data-bin/multi30k.en-de', source_lang='en', target_lang='de', load_alignments=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, image_feat_path=['data/vit_base_patch16_384'], image_feat_dim=[768], dropout=0.3, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=128, encoder_ffn_embed_dim=256, encoder_attention_heads=4, encoder_layers=4, decoder_embed_dim=128, decoder_ffn_embed_dim=256, decoder_attention_heads=4, decoder_layers=4, is_fusion_top=True, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=128, decoder_input_dim=128, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, distributed_num_procs=8)
2022-06-30 17:27:35 | INFO | fairseq.tasks.image_multimodal_translation | [en] dictionary: 9712 types
2022-06-30 17:27:35 | INFO | fairseq.tasks.image_multimodal_translation | [de] dictionary: 9712 types
2022-06-30 17:27:35 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.en
2022-06-30 17:27:35 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.de
2022-06-30 17:27:35 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de valid en-de 1014 examples
Traceback (most recent call last):
  File "/mnt/berry/home/yihang/pyenv/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq_cli/train.py", line 354, in cli_main
    distributed_utils.call_main(args, main)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/distributed_utils.py", line 283, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 3 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/distributed_utils.py", line 270, in distributed_main
    main(args, **kwargs)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq_cli/train.py", line 65, in main
    task.load_dataset(valid_sub_split, combine=False, epoch=1)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/tasks/image_multimodal_translation.py", line 314, in load_dataset
    self.datasets[split] = load_langpair_dataset(
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/tasks/image_multimodal_translation.py", line 154, in load_langpair_dataset
    img_dataset = ImageDataset(feat_pth_path, mask_pth_path)
  File "/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/fairseq/data/image_dataset.py", line 14, in __init__
    self.size = len([f for f in os.listdir(img_feat_path) if not f.startswith(".")])
NameError: name 'img_feat_path' is not defined

2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 6): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 7): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 4): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | distributed init (rank 5): tcp://localhost:12723
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 0
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 7
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 3
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 2
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 4
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 1
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 5
2022-06-30 17:30:13 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:30:13 | INFO | fairseq.distributed_utils | initialized host kubera as rank 6
2022-06-30 17:30:21 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, checkpoint_suffix='', checkpoint_shard_count=1, quantization_config_path=None, profile=False, criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='image_mmt', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=1000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=1000, batch_size_valid=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, distributed_world_size=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12723', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, fast_stat_sync=False, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', arch='image_multimodal_transformer_SA_top', max_epoch=0, max_update=8000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.005], min_lr=1e-09, use_bmuf=False, save_dir='checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=10, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, SA_image_dropout=0.1, SA_text_dropout=0, SA_attention_dropout=0.1, image_pre_norm=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, warmup_updates=2000, warmup_init_lr=1e-07, data='data-bin/multi30k.en-de', source_lang='en', target_lang='de', load_alignments=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, image_feat_path=['data/vit_base_patch16_384'], image_feat_dim=[768], dropout=0.3, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=128, encoder_ffn_embed_dim=256, encoder_attention_heads=4, encoder_layers=4, decoder_embed_dim=128, decoder_ffn_embed_dim=256, decoder_attention_heads=4, decoder_layers=4, is_fusion_top=True, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=128, decoder_input_dim=128, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, distributed_num_procs=8)
2022-06-30 17:30:21 | INFO | fairseq.tasks.image_multimodal_translation | [en] dictionary: 9712 types
2022-06-30 17:30:21 | INFO | fairseq.tasks.image_multimodal_translation | [de] dictionary: 9712 types
2022-06-30 17:30:21 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.en
2022-06-30 17:30:21 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.de
2022-06-30 17:30:21 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de valid en-de 1014 examples
2022-06-30 17:30:22 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9712, 128, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (selective_attns): ModuleList(
      (0): SelectiveAttention(
        (q_proj): Linear(in_features=128, out_features=128, bias=True)
        (k_proj): Linear(in_features=768, out_features=128, bias=True)
        (v_proj): Linear(in_features=768, out_features=128, bias=True)
        (attn_drop): Dropout(p=0.1, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (gate_denses): ModuleList(
      (0): Linear(in_features=256, out_features=128, bias=True)
    )
    (image_dropout_module): FairseqDropout()
    (text_dropout_module): FairseqDropout()
    (image_pre_norm_module): Identity()
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9712, 128, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=128, out_features=9712, bias=False)
  )
)
2022-06-30 17:30:22 | INFO | fairseq_cli.train | task: image_mmt (ImageMMTTask)
2022-06-30 17:30:22 | INFO | fairseq_cli.train | model: image_multimodal_transformer_SA_top (TransformerModel)
2022-06-30 17:30:22 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2022-06-30 17:30:22 | INFO | fairseq_cli.train | num. model params: 2830976 (num. trained: 2830976)
2022-06-30 17:30:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-06-30 17:30:22 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-06-30 17:30:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   4: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   5: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   6: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | rank   7: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:30:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2022-06-30 17:30:22 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2022-06-30 17:30:22 | INFO | fairseq_cli.train | max tokens per GPU = 1000 and max sentences per GPU = None
2022-06-30 17:30:22 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint_last.pt
2022-06-30 17:30:22 | INFO | fairseq.trainer | loading train data for epoch 1
2022-06-30 17:30:22 | INFO | fairseq.data.data_utils | loaded 29000 examples from: data-bin/multi30k.en-de/train.en-de.en
2022-06-30 17:30:22 | INFO | fairseq.data.data_utils | loaded 29000 examples from: data-bin/multi30k.en-de/train.en-de.de
2022-06-30 17:30:22 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de train en-de 29000 examples
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
2022-06-30 17:30:38 | INFO | fairseq.trainer | begin training epoch 1
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2022-06-30 17:32:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 17:33:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.429 | nll_loss 11.152 | ppl 2276.16 | wps 7339.3 | wpb 5172 | bsz 338 | num_updates 64
2022-06-30 17:33:52 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 17:33:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint1.pt (epoch 1 @ 64 updates, score 11.429) (writing took 1.6941069103777409 seconds)
2022-06-30 17:33:55 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-06-30 17:33:55 | INFO | train | epoch 001 | loss 12.726 | nll_loss 12.59 | ppl 6166.15 | wps 2684.1 | ups 0.4 | wpb 6711 | bsz 453.1 | num_updates 64 | lr 0.000160097 | gnorm 1.765 | loss_scale 128 | train_wall 50 | wall 212
2022-06-30 17:34:05 | INFO | fairseq.trainer | begin training epoch 2
2022-06-30 17:35:42 | INFO | train_inner | epoch 002:     36 / 64 loss=12.035, nll_loss=11.823, ppl=3624.18, wps=2520, ups=0.37, wpb=6735.2, bsz=455.2, num_updates=100, lr=0.000250095, gnorm=1.574, loss_scale=128, train_wall=108, wall=320
2022-06-30 17:36:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 17:36:56 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.19 | nll_loss 8.586 | ppl 384.15 | wps 9421.5 | wpb 5172 | bsz 338 | num_updates 128 | best_loss 9.19
2022-06-30 17:36:56 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 17:37:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint2.pt (epoch 2 @ 128 updates, score 9.19) (writing took 3.9570669662207365 seconds)
2022-06-30 17:37:00 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-06-30 17:37:00 | INFO | train | epoch 002 | loss 10.299 | nll_loss 9.88 | ppl 942.2 | wps 2318.2 | ups 0.35 | wpb 6711 | bsz 453.1 | num_updates 128 | lr 0.000320094 | gnorm 1.145 | loss_scale 128 | train_wall 76 | wall 398
2022-06-30 17:37:05 | INFO | fairseq.trainer | begin training epoch 3
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 7): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 4): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 6): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 5): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:12630
2022-06-30 17:42:39 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2022-06-30 17:42:39 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 0
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 3
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 5
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 2
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 4
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 7
2022-06-30 17:42:40 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 6
2022-06-30 17:42:40 | INFO | fairseq.distributed_utils | initialized host kubera as rank 1
2022-06-30 17:42:48 | INFO | fairseq_cli.train | Namespace(no_progress_bar=False, log_interval=100, log_format=None, tensorboard_logdir=None, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, min_loss_scale=0.0001, threshold_loss_scale=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, checkpoint_suffix='', checkpoint_shard_count=1, quantization_config_path=None, profile=False, criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='image_mmt', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=8000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=8000, batch_size_valid=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, distributed_world_size=8, distributed_rank=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:12630', distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='c10d', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, fast_stat_sync=False, broadcast_buffers=False, distributed_wrapper='DDP', slowmo_momentum=None, slowmo_algorithm='LocalSGD', localsgd_frequency=3, nprocs_per_node=8, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', arch='image_multimodal_transformer_SA_top', max_epoch=0, max_update=8000, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.005], min_lr=1e-09, use_bmuf=False, save_dir='checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0', restore_file='checkpoint_last.pt', finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_last_epochs=10, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, no_token_positional_embeddings=False, no_cross_attention=False, cross_self_attention=False, encoder_layerdrop=0, decoder_layerdrop=0, encoder_layers_to_keep=None, decoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, SA_image_dropout=0.1, SA_text_dropout=0, SA_attention_dropout=0.1, image_pre_norm=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, warmup_updates=2000, warmup_init_lr=1e-07, data='data-bin/multi30k.en-de', source_lang='en', target_lang='de', load_alignments=False, left_pad_source='True', left_pad_target='False', max_source_positions=1024, max_target_positions=1024, upsample_primary=1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_args=None, eval_bleu_print_samples=False, image_feat_path=['data/vit_base_patch16_384'], image_feat_dim=[768], dropout=0.3, share_all_embeddings=True, no_seed_provided=False, encoder_embed_dim=128, encoder_ffn_embed_dim=256, encoder_attention_heads=4, encoder_layers=4, decoder_embed_dim=128, decoder_ffn_embed_dim=256, decoder_attention_heads=4, decoder_layers=4, is_fusion_top=True, encoder_embed_path=None, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_decoder_input_output_embed=False, adaptive_input=False, decoder_output_dim=128, decoder_input_dim=128, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, distributed_num_procs=8)
2022-06-30 17:42:48 | INFO | fairseq.tasks.image_multimodal_translation | [en] dictionary: 9712 types
2022-06-30 17:42:48 | INFO | fairseq.tasks.image_multimodal_translation | [de] dictionary: 9712 types
2022-06-30 17:42:48 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.en
2022-06-30 17:42:48 | INFO | fairseq.data.data_utils | loaded 1014 examples from: data-bin/multi30k.en-de/valid.en-de.de
2022-06-30 17:42:48 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de valid en-de 1014 examples
2022-06-30 17:42:48 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9712, 128, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (selective_attns): ModuleList(
      (0): SelectiveAttention(
        (q_proj): Linear(in_features=128, out_features=128, bias=True)
        (k_proj): Linear(in_features=768, out_features=128, bias=True)
        (v_proj): Linear(in_features=768, out_features=128, bias=True)
        (attn_drop): Dropout(p=0.1, inplace=False)
        (proj): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (gate_denses): ModuleList(
      (0): Linear(in_features=256, out_features=128, bias=True)
    )
    (image_dropout_module): FairseqDropout()
    (text_dropout_module): FairseqDropout()
    (image_pre_norm_module): Identity()
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(9712, 128, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=128, out_features=128, bias=True)
          (v_proj): Linear(in_features=128, out_features=128, bias=True)
          (q_proj): Linear(in_features=128, out_features=128, bias=True)
          (out_proj): Linear(in_features=128, out_features=128, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=128, out_features=256, bias=True)
        (fc2): Linear(in_features=256, out_features=128, bias=True)
        (final_layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=128, out_features=9712, bias=False)
  )
)
2022-06-30 17:42:48 | INFO | fairseq_cli.train | task: image_mmt (ImageMMTTask)
2022-06-30 17:42:48 | INFO | fairseq_cli.train | model: image_multimodal_transformer_SA_top (TransformerModel)
2022-06-30 17:42:48 | INFO | fairseq_cli.train | criterion: label_smoothed_cross_entropy (LabelSmoothedCrossEntropyCriterion)
2022-06-30 17:42:48 | INFO | fairseq_cli.train | num. model params: 2830976 (num. trained: 2830976)
2022-06-30 17:42:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-06-30 17:42:49 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-06-30 17:42:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   1: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   2: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   3: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   4: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   5: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   6: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | rank   7: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-06-30 17:42:49 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2022-06-30 17:42:49 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2022-06-30 17:42:49 | INFO | fairseq_cli.train | max tokens per GPU = 8000 and max sentences per GPU = None
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py:552: UserWarning: The `check_reduction` argument in `DistributedDataParallel` module is deprecated. Please avoid using it.
  warnings.warn(
2022-06-30 17:42:50 | INFO | fairseq.trainer | loaded checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint_last.pt (epoch 3 @ 128 updates)
2022-06-30 17:42:50 | INFO | fairseq.trainer | loading train data for epoch 3
2022-06-30 17:42:50 | INFO | fairseq.data.data_utils | loaded 29000 examples from: data-bin/multi30k.en-de/train.en-de.en
2022-06-30 17:42:50 | INFO | fairseq.data.data_utils | loaded 29000 examples from: data-bin/multi30k.en-de/train.en-de.de
2022-06-30 17:42:50 | INFO | fairseq.tasks.image_multimodal_translation | data-bin/multi30k.en-de train en-de 29000 examples
2022-06-30 17:43:59 | INFO | fairseq.trainer | begin training epoch 3
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2022-06-30 17:45:16 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 64.0
2022-06-30 17:45:43 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 32.0
2022-06-30 17:45:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 17:48:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.086 | nll_loss 8.453 | ppl 350.36 | wps 0 | wpb 15516 | bsz 1014 | num_updates 135 | best_loss 9.086
2022-06-30 17:48:16 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 17:48:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint3.pt (epoch 3 @ 135 updates, score 9.086) (writing took 1.718237567692995 seconds)
2022-06-30 17:48:18 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-06-30 17:48:18 | INFO | train | epoch 003 | loss 9.807 | nll_loss 9.311 | ppl 635.22 | wps 1478.9 | ups 0.14 | wpb 10600.2 | bsz 716.1 | num_updates 135 | lr 0.000337593 | gnorm 1.112 | loss_scale 32 | train_wall 123 | wall 0
2022-06-30 17:49:31 | INFO | fairseq.trainer | begin training epoch 4
2022-06-30 17:51:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 17:53:05 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.941 | nll_loss 8.271 | ppl 308.87 | wps 0 | wpb 15516 | bsz 1014 | num_updates 144 | best_loss 8.941
2022-06-30 17:53:05 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 17:53:06 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint4.pt (epoch 4 @ 144 updates, score 8.941) (writing took 1.7549010012298822 seconds)
2022-06-30 17:53:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-06-30 17:53:06 | INFO | train | epoch 004 | loss 8.992 | nll_loss 8.359 | ppl 328.44 | wps 1486.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 144 | lr 0.000360093 | gnorm 0.625 | loss_scale 32 | train_wall 60 | wall 0
2022-06-30 17:54:42 | INFO | fairseq.trainer | begin training epoch 5
2022-06-30 17:55:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 17:57:50 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.834 | nll_loss 8.129 | ppl 279.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 153 | best_loss 8.834
2022-06-30 17:57:50 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 17:57:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint5.pt (epoch 5 @ 153 updates, score 8.834) (writing took 1.73284344188869 seconds)
2022-06-30 17:57:51 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-06-30 17:57:51 | INFO | train | epoch 005 | loss 8.861 | nll_loss 8.196 | ppl 293.28 | wps 1507.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 153 | lr 0.000382592 | gnorm 0.852 | loss_scale 32 | train_wall 31 | wall 0
2022-06-30 17:59:24 | INFO | fairseq.trainer | begin training epoch 6
2022-06-30 18:01:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:03:07 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.726 | nll_loss 7.995 | ppl 255.2 | wps 0 | wpb 15516 | bsz 1014 | num_updates 162 | best_loss 8.726
2022-06-30 18:03:07 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:03:08 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint6.pt (epoch 6 @ 162 updates, score 8.726) (writing took 1.7362216990441084 seconds)
2022-06-30 18:03:08 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-06-30 18:03:08 | INFO | train | epoch 006 | loss 8.748 | nll_loss 8.055 | ppl 266.01 | wps 1354.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 162 | lr 0.000405092 | gnorm 0.591 | loss_scale 32 | train_wall 54 | wall 0
2022-06-30 18:04:50 | INFO | fairseq.trainer | begin training epoch 7
2022-06-30 18:06:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:08:17 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.625 | nll_loss 7.869 | ppl 233.75 | wps 0 | wpb 15516 | bsz 1014 | num_updates 171 | best_loss 8.625
2022-06-30 18:08:17 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:08:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint7.pt (epoch 7 @ 171 updates, score 8.625) (writing took 1.9983753077685833 seconds)
2022-06-30 18:08:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-06-30 18:08:19 | INFO | train | epoch 007 | loss 8.645 | nll_loss 7.929 | ppl 243.65 | wps 1383.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 171 | lr 0.000427591 | gnorm 0.58 | loss_scale 32 | train_wall 3 | wall 0
2022-06-30 18:09:55 | INFO | fairseq.trainer | begin training epoch 8
2022-06-30 18:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:12:59 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.591 | nll_loss 7.823 | ppl 226.47 | wps 0 | wpb 15516 | bsz 1014 | num_updates 180 | best_loss 8.591
2022-06-30 18:12:59 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:13:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint8.pt (epoch 8 @ 180 updates, score 8.591) (writing took 1.6518368609249592 seconds)
2022-06-30 18:13:00 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-06-30 18:13:00 | INFO | train | epoch 008 | loss 8.552 | nll_loss 7.815 | ppl 225.17 | wps 1525.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 180 | lr 0.000450091 | gnorm 0.772 | loss_scale 32 | train_wall 36 | wall 0
2022-06-30 18:14:36 | INFO | fairseq.trainer | begin training epoch 9
2022-06-30 18:15:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:18:02 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.435 | nll_loss 7.642 | ppl 199.79 | wps 0 | wpb 15516 | bsz 1014 | num_updates 189 | best_loss 8.435
2022-06-30 18:18:02 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:18:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint9.pt (epoch 9 @ 189 updates, score 8.435) (writing took 1.730801735073328 seconds)
2022-06-30 18:18:04 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-06-30 18:18:04 | INFO | train | epoch 009 | loss 8.474 | nll_loss 7.722 | ppl 211.1 | wps 1416.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 189 | lr 0.000472591 | gnorm 0.9 | loss_scale 32 | train_wall 30 | wall 0
2022-06-30 18:19:39 | INFO | fairseq.trainer | begin training epoch 10
2022-06-30 18:21:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:23:16 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.349 | nll_loss 7.541 | ppl 186.21 | wps 0 | wpb 15516 | bsz 1014 | num_updates 198 | best_loss 8.349
2022-06-30 18:23:17 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:23:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint10.pt (epoch 10 @ 198 updates, score 8.349) (writing took 1.6676111444830894 seconds)
2022-06-30 18:23:18 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-06-30 18:23:18 | INFO | train | epoch 010 | loss 8.377 | nll_loss 7.608 | ppl 195.07 | wps 1364.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 198 | lr 0.00049509 | gnorm 0.635 | loss_scale 32 | train_wall 21 | wall 0
2022-06-30 18:24:54 | INFO | fairseq.trainer | begin training epoch 11
2022-06-30 18:25:47 | INFO | train_inner | epoch 011:      2 / 9 loss=8.747, nll_loss=8.055, ppl=266.01, wps=1362.6, ups=0.04, wpb=36119.4, bsz=2439.4, num_updates=200, lr=0.00050009, gnorm=0.802, loss_scale=32, train_wall=301, wall=0
2022-06-30 18:26:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:28:11 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.288 | nll_loss 7.472 | ppl 177.52 | wps 0 | wpb 15516 | bsz 1014 | num_updates 207 | best_loss 8.288
2022-06-30 18:28:11 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:28:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint11.pt (epoch 11 @ 207 updates, score 8.288) (writing took 1.6606301236897707 seconds)
2022-06-30 18:28:13 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-06-30 18:28:13 | INFO | train | epoch 011 | loss 8.291 | nll_loss 7.509 | ppl 182.18 | wps 1458.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 207 | lr 0.00051759 | gnorm 0.622 | loss_scale 32 | train_wall 6 | wall 0
2022-06-30 18:29:38 | INFO | fairseq.trainer | begin training epoch 12
2022-06-30 18:31:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:33:20 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.18 | nll_loss 7.345 | ppl 162.57 | wps 0 | wpb 15516 | bsz 1014 | num_updates 216 | best_loss 8.18
2022-06-30 18:33:21 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:33:22 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint12.pt (epoch 12 @ 216 updates, score 8.18) (writing took 1.6397780310362577 seconds)
2022-06-30 18:33:22 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-06-30 18:33:22 | INFO | train | epoch 012 | loss 8.212 | nll_loss 7.419 | ppl 171.09 | wps 1388.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 216 | lr 0.000540089 | gnorm 0.797 | loss_scale 32 | train_wall 44 | wall 0
2022-06-30 18:34:39 | INFO | fairseq.trainer | begin training epoch 13
2022-06-30 18:36:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:37:58 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.082 | nll_loss 7.235 | ppl 150.68 | wps 0 | wpb 15516 | bsz 1014 | num_updates 225 | best_loss 8.082
2022-06-30 18:37:58 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:38:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint13.pt (epoch 13 @ 225 updates, score 8.082) (writing took 1.670783033594489 seconds)
2022-06-30 18:38:00 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-06-30 18:38:00 | INFO | train | epoch 013 | loss 8.12 | nll_loss 7.312 | ppl 158.86 | wps 1547.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 225 | lr 0.000562589 | gnorm 0.817 | loss_scale 32 | train_wall 51 | wall 0
2022-06-30 18:39:35 | INFO | fairseq.trainer | begin training epoch 14
2022-06-30 18:41:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:43:46 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.998 | nll_loss 7.129 | ppl 139.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 234 | best_loss 7.998
2022-06-30 18:43:46 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:43:48 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint14.pt (epoch 14 @ 234 updates, score 7.998) (writing took 1.965952791273594 seconds)
2022-06-30 18:43:48 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-06-30 18:43:48 | INFO | train | epoch 014 | loss 8.028 | nll_loss 7.207 | ppl 147.74 | wps 1231.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 234 | lr 0.000585088 | gnorm 0.662 | loss_scale 32 | train_wall 35 | wall 0
2022-06-30 18:45:13 | INFO | fairseq.trainer | begin training epoch 15
2022-06-30 18:46:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:48:34 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.912 | nll_loss 7.029 | ppl 130.58 | wps 0 | wpb 15516 | bsz 1014 | num_updates 243 | best_loss 7.912
2022-06-30 18:48:34 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:48:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint15.pt (epoch 15 @ 243 updates, score 7.912) (writing took 1.6553021539002657 seconds)
2022-06-30 18:48:36 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-06-30 18:48:36 | INFO | train | epoch 015 | loss 7.936 | nll_loss 7.098 | ppl 137.04 | wps 1495 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 243 | lr 0.000607588 | gnorm 0.692 | loss_scale 32 | train_wall 48 | wall 0
2022-06-30 18:50:10 | INFO | fairseq.trainer | begin training epoch 16
2022-06-30 18:51:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:54:10 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.794 | nll_loss 6.908 | ppl 120.08 | wps 0 | wpb 15516 | bsz 1014 | num_updates 252 | best_loss 7.794
2022-06-30 18:54:10 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:54:12 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint16.pt (epoch 16 @ 252 updates, score 7.794) (writing took 1.7642608098685741 seconds)
2022-06-30 18:54:12 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-06-30 18:54:12 | INFO | train | epoch 016 | loss 7.85 | nll_loss 7 | ppl 128.01 | wps 1277.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 252 | lr 0.000630087 | gnorm 0.927 | loss_scale 32 | train_wall 31 | wall 0
2022-06-30 18:55:42 | INFO | fairseq.trainer | begin training epoch 17
2022-06-30 18:57:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 18:59:02 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.7 | nll_loss 6.783 | ppl 110.1 | wps 0 | wpb 15516 | bsz 1014 | num_updates 261 | best_loss 7.7
2022-06-30 18:59:02 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 18:59:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint17.pt (epoch 17 @ 261 updates, score 7.7) (writing took 1.7429109085351229 seconds)
2022-06-30 18:59:04 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-06-30 18:59:04 | INFO | train | epoch 017 | loss 7.748 | nll_loss 6.883 | ppl 118 | wps 1471.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 261 | lr 0.000652587 | gnorm 0.709 | loss_scale 32 | train_wall 36 | wall 0
2022-06-30 19:00:39 | INFO | fairseq.trainer | begin training epoch 18
2022-06-30 19:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:04:09 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.581 | nll_loss 6.656 | ppl 100.81 | wps 0 | wpb 15516 | bsz 1014 | num_updates 270 | best_loss 7.581
2022-06-30 19:04:09 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:04:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint18.pt (epoch 18 @ 270 updates, score 7.581) (writing took 1.697152415290475 seconds)
2022-06-30 19:04:11 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-06-30 19:04:11 | INFO | train | epoch 018 | loss 7.65 | nll_loss 6.767 | ppl 108.92 | wps 1400.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 270 | lr 0.000675086 | gnorm 0.695 | loss_scale 32 | train_wall 15 | wall 0
2022-06-30 19:05:46 | INFO | fairseq.trainer | begin training epoch 19
2022-06-30 19:07:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:09:09 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.505 | nll_loss 6.555 | ppl 93.99 | wps 0 | wpb 15516 | bsz 1014 | num_updates 279 | best_loss 7.505
2022-06-30 19:09:10 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:09:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint19.pt (epoch 19 @ 279 updates, score 7.505) (writing took 1.694903176277876 seconds)
2022-06-30 19:09:11 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-06-30 19:09:11 | INFO | train | epoch 019 | loss 7.563 | nll_loss 6.667 | ppl 101.61 | wps 1428.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 279 | lr 0.000697586 | gnorm 0.843 | loss_scale 32 | train_wall 36 | wall 0
2022-06-30 19:10:47 | INFO | fairseq.trainer | begin training epoch 20
2022-06-30 19:11:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:13:47 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.394 | nll_loss 6.428 | ppl 86.1 | wps 0 | wpb 15516 | bsz 1014 | num_updates 288 | best_loss 7.394
2022-06-30 19:13:47 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:13:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint20.pt (epoch 20 @ 288 updates, score 7.394) (writing took 1.620840972289443 seconds)
2022-06-30 19:13:49 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-06-30 19:13:49 | INFO | train | epoch 020 | loss 7.457 | nll_loss 6.543 | ppl 93.27 | wps 1545.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 288 | lr 0.000720086 | gnorm 0.62 | loss_scale 32 | train_wall 9 | wall 0
2022-06-30 19:15:25 | INFO | fairseq.trainer | begin training epoch 21
2022-06-30 19:16:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:18:17 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.295 | nll_loss 6.314 | ppl 79.58 | wps 0 | wpb 15516 | bsz 1014 | num_updates 297 | best_loss 7.295
2022-06-30 19:18:17 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:18:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint21.pt (epoch 21 @ 297 updates, score 7.295) (writing took 1.7571375630795956 seconds)
2022-06-30 19:18:19 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-06-30 19:18:19 | INFO | train | epoch 021 | loss 7.361 | nll_loss 6.433 | ppl 86.37 | wps 1590 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 297 | lr 0.000742585 | gnorm 0.665 | loss_scale 32 | train_wall 28 | wall 0
2022-06-30 19:19:55 | INFO | fairseq.trainer | begin training epoch 22
2022-06-30 19:21:03 | INFO | train_inner | epoch 022:      3 / 9 loss=7.811, nll_loss=6.954, ppl=124.01, wps=1437.3, ups=0.03, wpb=47668.8, bsz=3221.8, num_updates=300, lr=0.000750085, gnorm=0.739, loss_scale=32, train_wall=368, wall=0
2022-06-30 19:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:23:33 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.215 | nll_loss 6.221 | ppl 74.57 | wps 0 | wpb 15516 | bsz 1014 | num_updates 306 | best_loss 7.215
2022-06-30 19:23:33 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:23:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint22.pt (epoch 22 @ 306 updates, score 7.215) (writing took 1.6693299356848001 seconds)
2022-06-30 19:23:35 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-06-30 19:23:35 | INFO | train | epoch 022 | loss 7.276 | nll_loss 6.331 | ppl 80.51 | wps 1362 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 306 | lr 0.000765085 | gnorm 0.802 | loss_scale 32 | train_wall 61 | wall 0
2022-06-30 19:25:10 | INFO | fairseq.trainer | begin training epoch 23
2022-06-30 19:26:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:28:45 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.14 | nll_loss 6.13 | ppl 70.06 | wps 0 | wpb 15516 | bsz 1014 | num_updates 315 | best_loss 7.14
2022-06-30 19:28:45 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:28:48 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint23.pt (epoch 23 @ 315 updates, score 7.14) (writing took 2.985335413366556 seconds)
2022-06-30 19:28:48 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-06-30 19:28:48 | INFO | train | epoch 023 | loss 7.196 | nll_loss 6.237 | ppl 75.45 | wps 1370.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 315 | lr 0.000787584 | gnorm 0.98 | loss_scale 32 | train_wall 33 | wall 0
2022-06-30 19:30:24 | INFO | fairseq.trainer | begin training epoch 24
2022-06-30 19:32:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:34:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.004 | nll_loss 5.969 | ppl 62.65 | wps 0 | wpb 15516 | bsz 1014 | num_updates 324 | best_loss 7.004
2022-06-30 19:34:13 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:34:15 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint24.pt (epoch 24 @ 324 updates, score 7.004) (writing took 1.6239634212106466 seconds)
2022-06-30 19:34:15 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-06-30 19:34:15 | INFO | train | epoch 024 | loss 7.1 | nll_loss 6.125 | ppl 69.8 | wps 1313.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 324 | lr 0.000810084 | gnorm 0.79 | loss_scale 32 | train_wall 11 | wall 0
2022-06-30 19:35:45 | INFO | fairseq.trainer | begin training epoch 25
2022-06-30 19:37:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:39:25 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.891 | nll_loss 5.845 | ppl 57.49 | wps 0 | wpb 15516 | bsz 1014 | num_updates 333 | best_loss 6.891
2022-06-30 19:39:25 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:39:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint25.pt (epoch 25 @ 333 updates, score 6.891) (writing took 1.5234873443841934 seconds)
2022-06-30 19:39:27 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-06-30 19:39:27 | INFO | train | epoch 025 | loss 6.993 | nll_loss 6.001 | ppl 64.05 | wps 1377 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 333 | lr 0.000832583 | gnorm 0.669 | loss_scale 32 | train_wall 41 | wall 0
2022-06-30 19:40:59 | INFO | fairseq.trainer | begin training epoch 26
2022-06-30 19:42:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:44:30 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.824 | nll_loss 5.75 | ppl 53.82 | wps 0 | wpb 15516 | bsz 1014 | num_updates 342 | best_loss 6.824
2022-06-30 19:44:30 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:44:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint26.pt (epoch 26 @ 342 updates, score 6.824) (writing took 1.5635573137551546 seconds)
2022-06-30 19:44:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-06-30 19:44:31 | INFO | train | epoch 026 | loss 6.899 | nll_loss 5.89 | ppl 59.3 | wps 1410.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 342 | lr 0.000855083 | gnorm 0.708 | loss_scale 32 | train_wall 37 | wall 0
2022-06-30 19:46:07 | INFO | fairseq.trainer | begin training epoch 27
2022-06-30 19:47:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:49:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.743 | nll_loss 5.668 | ppl 50.85 | wps 0 | wpb 15516 | bsz 1014 | num_updates 351 | best_loss 6.743
2022-06-30 19:49:30 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:49:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint27.pt (epoch 27 @ 351 updates, score 6.743) (writing took 1.5359164346009493 seconds)
2022-06-30 19:49:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-06-30 19:49:32 | INFO | train | epoch 027 | loss 6.803 | nll_loss 5.776 | ppl 54.79 | wps 1431.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 351 | lr 0.000877582 | gnorm 0.727 | loss_scale 32 | train_wall 3 | wall 0
2022-06-30 19:50:34 | INFO | fairseq.trainer | begin training epoch 28
2022-06-30 19:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:54:10 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.61 | nll_loss 5.504 | ppl 45.39 | wps 0 | wpb 15516 | bsz 1014 | num_updates 360 | best_loss 6.61
2022-06-30 19:54:10 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:54:12 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint28.pt (epoch 28 @ 360 updates, score 6.61) (writing took 1.5753207728266716 seconds)
2022-06-30 19:54:12 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-06-30 19:54:12 | INFO | train | epoch 028 | loss 6.715 | nll_loss 5.672 | ppl 50.97 | wps 1532.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 360 | lr 0.000900082 | gnorm 0.842 | loss_scale 32 | train_wall 56 | wall 0
2022-06-30 19:55:13 | INFO | fairseq.trainer | begin training epoch 29
2022-06-30 19:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 19:59:49 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.562 | nll_loss 5.429 | ppl 43.08 | wps 0 | wpb 15516 | bsz 1014 | num_updates 369 | best_loss 6.562
2022-06-30 19:59:49 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 19:59:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint29.pt (epoch 29 @ 369 updates, score 6.562) (writing took 1.7214365247637033 seconds)
2022-06-30 19:59:51 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-06-30 19:59:51 | INFO | train | epoch 029 | loss 6.628 | nll_loss 5.571 | ppl 47.55 | wps 1265.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 369 | lr 0.000922582 | gnorm 0.851 | loss_scale 32 | train_wall 99 | wall 0
2022-06-30 20:01:17 | INFO | fairseq.trainer | begin training epoch 30
2022-06-30 20:03:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:04:49 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.434 | nll_loss 5.284 | ppl 38.97 | wps 0 | wpb 15516 | bsz 1014 | num_updates 378 | best_loss 6.434
2022-06-30 20:04:49 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:04:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint30.pt (epoch 30 @ 378 updates, score 6.434) (writing took 1.5213121641427279 seconds)
2022-06-30 20:04:51 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-06-30 20:04:51 | INFO | train | epoch 030 | loss 6.532 | nll_loss 5.457 | ppl 43.93 | wps 1433.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 378 | lr 0.000945081 | gnorm 0.762 | loss_scale 32 | train_wall 44 | wall 0
2022-06-30 20:06:26 | INFO | fairseq.trainer | begin training epoch 31
2022-06-30 20:08:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:10:44 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.334 | nll_loss 5.166 | ppl 35.91 | wps 0 | wpb 15516 | bsz 1014 | num_updates 387 | best_loss 6.334
2022-06-30 20:10:44 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:10:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint31.pt (epoch 31 @ 387 updates, score 6.334) (writing took 1.562435695901513 seconds)
2022-06-30 20:10:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-06-30 20:10:46 | INFO | train | epoch 031 | loss 6.442 | nll_loss 5.351 | ppl 40.82 | wps 1209.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 387 | lr 0.000967581 | gnorm 0.786 | loss_scale 32 | train_wall 38 | wall 0
2022-06-30 20:12:16 | INFO | fairseq.trainer | begin training epoch 32
2022-06-30 20:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:15:37 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.256 | nll_loss 5.066 | ppl 33.49 | wps 0 | wpb 15516 | bsz 1014 | num_updates 396 | best_loss 6.256
2022-06-30 20:15:37 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:15:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint32.pt (epoch 32 @ 396 updates, score 6.256) (writing took 1.5590595752000809 seconds)
2022-06-30 20:15:39 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-06-30 20:15:39 | INFO | train | epoch 032 | loss 6.353 | nll_loss 5.248 | ppl 37.99 | wps 1466.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 396 | lr 0.00099008 | gnorm 0.76 | loss_scale 32 | train_wall 37 | wall 0
2022-06-30 20:17:15 | INFO | fairseq.trainer | begin training epoch 33
2022-06-30 20:18:26 | INFO | train_inner | epoch 033:      4 / 9 loss=6.777, nll_loss=5.746, ppl=53.68, wps=1387.5, ups=0.03, wpb=47768.3, bsz=3225.8, num_updates=400, lr=0.00100008, gnorm=0.78, loss_scale=32, train_wall=437, wall=0
2022-06-30 20:18:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:21:24 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.152 | nll_loss 4.941 | ppl 30.73 | wps 0 | wpb 15516 | bsz 1014 | num_updates 405 | best_loss 6.152
2022-06-30 20:21:24 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:21:25 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint33.pt (epoch 33 @ 405 updates, score 6.152) (writing took 1.5501270350068808 seconds)
2022-06-30 20:21:25 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-06-30 20:21:25 | INFO | train | epoch 033 | loss 6.255 | nll_loss 5.133 | ppl 35.08 | wps 1238.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 405 | lr 0.00101258 | gnorm 0.64 | loss_scale 32 | train_wall 8 | wall 0
2022-06-30 20:22:38 | INFO | fairseq.trainer | begin training epoch 34
2022-06-30 20:24:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:26:53 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.117 | nll_loss 4.895 | ppl 29.76 | wps 0 | wpb 15516 | bsz 1014 | num_updates 414 | best_loss 6.117
2022-06-30 20:26:53 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:26:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint34.pt (epoch 34 @ 414 updates, score 6.117) (writing took 1.506580026820302 seconds)
2022-06-30 20:26:55 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-06-30 20:26:55 | INFO | train | epoch 034 | loss 6.169 | nll_loss 5.033 | ppl 32.74 | wps 1304.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 414 | lr 0.00103508 | gnorm 0.697 | loss_scale 32 | train_wall 82 | wall 0
2022-06-30 20:28:30 | INFO | fairseq.trainer | begin training epoch 35
2022-06-30 20:29:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:31:35 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.002 | nll_loss 4.755 | ppl 27.01 | wps 0 | wpb 15516 | bsz 1014 | num_updates 423 | best_loss 6.002
2022-06-30 20:31:35 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:31:37 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint35.pt (epoch 35 @ 423 updates, score 6.002) (writing took 1.5958395432680845 seconds)
2022-06-30 20:31:37 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-06-30 20:31:37 | INFO | train | epoch 035 | loss 6.088 | nll_loss 4.937 | ppl 30.64 | wps 1522.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 423 | lr 0.00105758 | gnorm 0.73 | loss_scale 32 | train_wall 28 | wall 0
2022-06-30 20:32:35 | INFO | fairseq.trainer | begin training epoch 36
2022-06-30 20:35:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:36:50 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.947 | nll_loss 4.685 | ppl 25.72 | wps 0 | wpb 15516 | bsz 1014 | num_updates 432 | best_loss 5.947
2022-06-30 20:36:50 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:36:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint36.pt (epoch 36 @ 432 updates, score 5.947) (writing took 1.5813745241612196 seconds)
2022-06-30 20:36:52 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-06-30 20:36:52 | INFO | train | epoch 036 | loss 6.004 | nll_loss 4.838 | ppl 28.59 | wps 1363.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 432 | lr 0.00108008 | gnorm 0.688 | loss_scale 32 | train_wall 107 | wall 0
2022-06-30 20:38:23 | INFO | fairseq.trainer | begin training epoch 37
2022-06-30 20:40:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:41:53 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.831 | nll_loss 4.542 | ppl 23.3 | wps 0 | wpb 15516 | bsz 1014 | num_updates 441 | best_loss 5.831
2022-06-30 20:41:53 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:41:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint37.pt (epoch 37 @ 441 updates, score 5.831) (writing took 1.538747014477849 seconds)
2022-06-30 20:41:55 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-06-30 20:41:55 | INFO | train | epoch 037 | loss 5.921 | nll_loss 4.741 | ppl 26.74 | wps 1417 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 441 | lr 0.00110258 | gnorm 0.704 | loss_scale 32 | train_wall 50 | wall 0
2022-06-30 20:43:02 | INFO | fairseq.trainer | begin training epoch 38
2022-06-30 20:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:47:28 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.784 | nll_loss 4.494 | ppl 22.53 | wps 0 | wpb 15516 | bsz 1014 | num_updates 450 | best_loss 5.784
2022-06-30 20:47:29 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:47:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint38.pt (epoch 38 @ 450 updates, score 5.784) (writing took 1.6399025954306126 seconds)
2022-06-30 20:47:30 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-06-30 20:47:30 | INFO | train | epoch 038 | loss 5.835 | nll_loss 4.639 | ppl 24.91 | wps 1280.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 450 | lr 0.00112508 | gnorm 0.627 | loss_scale 32 | train_wall 87 | wall 0
2022-06-30 20:49:05 | INFO | fairseq.trainer | begin training epoch 39
2022-06-30 20:50:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:52:17 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.682 | nll_loss 4.364 | ppl 20.59 | wps 0 | wpb 15516 | bsz 1014 | num_updates 459 | best_loss 5.682
2022-06-30 20:52:17 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:52:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint39.pt (epoch 39 @ 459 updates, score 5.682) (writing took 1.5372762642800808 seconds)
2022-06-30 20:52:19 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-06-30 20:52:19 | INFO | train | epoch 039 | loss 5.763 | nll_loss 4.556 | ppl 23.52 | wps 1489.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 459 | lr 0.00114758 | gnorm 0.741 | loss_scale 32 | train_wall 35 | wall 0
2022-06-30 20:53:21 | INFO | fairseq.trainer | begin training epoch 40
2022-06-30 20:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 20:56:49 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.612 | nll_loss 4.282 | ppl 19.45 | wps 0 | wpb 15516 | bsz 1014 | num_updates 468 | best_loss 5.612
2022-06-30 20:56:50 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 20:56:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint40.pt (epoch 40 @ 468 updates, score 5.612) (writing took 1.5344843547791243 seconds)
2022-06-30 20:56:51 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-06-30 20:56:51 | INFO | train | epoch 040 | loss 5.676 | nll_loss 4.452 | ppl 21.89 | wps 1576 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 468 | lr 0.00117008 | gnorm 0.605 | loss_scale 32 | train_wall 67 | wall 0
2022-06-30 20:58:18 | INFO | fairseq.trainer | begin training epoch 41
2022-06-30 20:59:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:01:24 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.537 | nll_loss 4.19 | ppl 18.25 | wps 0 | wpb 15516 | bsz 1014 | num_updates 477 | best_loss 5.537
2022-06-30 21:01:24 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:01:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint41.pt (epoch 41 @ 477 updates, score 5.537) (writing took 1.5259151626378298 seconds)
2022-06-30 21:01:26 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-06-30 21:01:26 | INFO | train | epoch 041 | loss 5.621 | nll_loss 4.384 | ppl 20.89 | wps 1563.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 477 | lr 0.00119258 | gnorm 0.794 | loss_scale 32 | train_wall 37 | wall 0
2022-06-30 21:02:44 | INFO | fairseq.trainer | begin training epoch 42
2022-06-30 21:04:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:06:03 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.518 | nll_loss 4.17 | ppl 18 | wps 0 | wpb 15516 | bsz 1014 | num_updates 486 | best_loss 5.518
2022-06-30 21:06:03 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:06:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint42.pt (epoch 42 @ 486 updates, score 5.518) (writing took 1.6860714759677649 seconds)
2022-06-30 21:06:05 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-06-30 21:06:05 | INFO | train | epoch 042 | loss 5.549 | nll_loss 4.301 | ppl 19.71 | wps 1540.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 486 | lr 0.00121508 | gnorm 0.815 | loss_scale 32 | train_wall 50 | wall 0
2022-06-30 21:07:39 | INFO | fairseq.trainer | begin training epoch 43
2022-06-30 21:08:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:10:48 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.464 | nll_loss 4.094 | ppl 17.08 | wps 0 | wpb 15516 | bsz 1014 | num_updates 495 | best_loss 5.464
2022-06-30 21:10:48 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:10:50 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint43.pt (epoch 43 @ 495 updates, score 5.464) (writing took 1.5724801551550627 seconds)
2022-06-30 21:10:50 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-06-30 21:10:50 | INFO | train | epoch 043 | loss 5.49 | nll_loss 4.231 | ppl 18.77 | wps 1506.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 495 | lr 0.00123758 | gnorm 0.846 | loss_scale 32 | train_wall 11 | wall 0
2022-06-30 21:12:25 | INFO | fairseq.trainer | begin training epoch 44
2022-06-30 21:13:28 | INFO | train_inner | epoch 044:      5 / 9 loss=5.817, nll_loss=4.618, ppl=24.55, wps=1449.5, ups=0.03, wpb=47868, bsz=3225.3, num_updates=500, lr=0.00125008, gnorm=0.729, loss_scale=32, train_wall=581, wall=0
2022-06-30 21:13:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:15:29 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.385 | nll_loss 4 | ppl 16 | wps 0 | wpb 15516 | bsz 1014 | num_updates 504 | best_loss 5.385
2022-06-30 21:15:29 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:15:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint44.pt (epoch 44 @ 504 updates, score 5.385) (writing took 1.6048071160912514 seconds)
2022-06-30 21:15:31 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-06-30 21:15:31 | INFO | train | epoch 044 | loss 5.429 | nll_loss 4.158 | ppl 17.85 | wps 1526.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 504 | lr 0.00126007 | gnorm 0.849 | loss_scale 32 | train_wall 32 | wall 0
2022-06-30 21:17:06 | INFO | fairseq.trainer | begin training epoch 45
2022-06-30 21:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:20:09 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.345 | nll_loss 3.94 | ppl 15.35 | wps 0 | wpb 15516 | bsz 1014 | num_updates 513 | best_loss 5.345
2022-06-30 21:20:09 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:20:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint45.pt (epoch 45 @ 513 updates, score 5.345) (writing took 1.591914052143693 seconds)
2022-06-30 21:20:11 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-06-30 21:20:11 | INFO | train | epoch 045 | loss 5.348 | nll_loss 4.063 | ppl 16.72 | wps 1535.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 513 | lr 0.00128257 | gnorm 0.626 | loss_scale 32 | train_wall 7 | wall 0
2022-06-30 21:21:46 | INFO | fairseq.trainer | begin training epoch 46
2022-06-30 21:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:24:37 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.301 | nll_loss 3.893 | ppl 14.86 | wps 0 | wpb 15516 | bsz 1014 | num_updates 522 | best_loss 5.301
2022-06-30 21:24:37 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:24:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint46.pt (epoch 46 @ 522 updates, score 5.301) (writing took 1.5502231866121292 seconds)
2022-06-30 21:24:39 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-06-30 21:24:39 | INFO | train | epoch 046 | loss 5.293 | nll_loss 3.997 | ppl 15.96 | wps 1602.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 522 | lr 0.00130507 | gnorm 0.738 | loss_scale 32 | train_wall 25 | wall 0
2022-06-30 21:25:54 | INFO | fairseq.trainer | begin training epoch 47
2022-06-30 21:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:29:20 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.228 | nll_loss 3.799 | ppl 13.92 | wps 0 | wpb 15516 | bsz 1014 | num_updates 531 | best_loss 5.228
2022-06-30 21:29:21 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:29:22 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint47.pt (epoch 47 @ 531 updates, score 5.228) (writing took 1.5419398993253708 seconds)
2022-06-30 21:29:22 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-06-30 21:29:22 | INFO | train | epoch 047 | loss 5.222 | nll_loss 3.912 | ppl 15.06 | wps 1516.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 531 | lr 0.00132757 | gnorm 0.49 | loss_scale 32 | train_wall 53 | wall 0
2022-06-30 21:30:57 | INFO | fairseq.trainer | begin training epoch 48
2022-06-30 21:32:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:34:15 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.218 | nll_loss 3.772 | ppl 13.66 | wps 0 | wpb 15516 | bsz 1014 | num_updates 540 | best_loss 5.218
2022-06-30 21:34:15 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:34:17 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint48.pt (epoch 48 @ 540 updates, score 5.218) (writing took 1.847971698269248 seconds)
2022-06-30 21:34:17 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-06-30 21:34:17 | INFO | train | epoch 048 | loss 5.176 | nll_loss 3.857 | ppl 14.49 | wps 1458.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 540 | lr 0.00135007 | gnorm 0.723 | loss_scale 32 | train_wall 27 | wall 0
2022-06-30 21:35:55 | INFO | fairseq.trainer | begin training epoch 49
2022-06-30 21:37:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:38:51 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.182 | nll_loss 3.726 | ppl 13.23 | wps 0 | wpb 15516 | bsz 1014 | num_updates 549 | best_loss 5.182
2022-06-30 21:38:51 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:38:53 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint49.pt (epoch 49 @ 549 updates, score 5.182) (writing took 1.5699920076876879 seconds)
2022-06-30 21:38:53 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-06-30 21:38:53 | INFO | train | epoch 049 | loss 5.145 | nll_loss 3.82 | ppl 14.12 | wps 1556.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 549 | lr 0.00137257 | gnorm 0.935 | loss_scale 32 | train_wall 28 | wall 0
2022-06-30 21:40:08 | INFO | fairseq.trainer | begin training epoch 50
2022-06-30 21:41:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:43:23 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.136 | nll_loss 3.68 | ppl 12.82 | wps 0 | wpb 15516 | bsz 1014 | num_updates 558 | best_loss 5.136
2022-06-30 21:43:23 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:43:24 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint50.pt (epoch 50 @ 558 updates, score 5.136) (writing took 1.5526943039149046 seconds)
2022-06-30 21:43:24 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-06-30 21:43:24 | INFO | train | epoch 050 | loss 5.08 | nll_loss 3.741 | ppl 13.38 | wps 1580.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 558 | lr 0.00139507 | gnorm 0.688 | loss_scale 32 | train_wall 46 | wall 0
2022-06-30 21:44:59 | INFO | fairseq.trainer | begin training epoch 51
2022-06-30 21:46:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:47:51 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.086 | nll_loss 3.612 | ppl 12.23 | wps 0 | wpb 15516 | bsz 1014 | num_updates 567 | best_loss 5.086
2022-06-30 21:47:52 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:47:53 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint51.pt (epoch 51 @ 567 updates, score 5.086) (writing took 1.6116409078240395 seconds)
2022-06-30 21:47:53 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-06-30 21:47:53 | INFO | train | epoch 051 | loss 5.011 | nll_loss 3.661 | ppl 12.65 | wps 1597.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 567 | lr 0.00141757 | gnorm 0.492 | loss_scale 32 | train_wall 28 | wall 0
2022-06-30 21:48:53 | INFO | fairseq.trainer | begin training epoch 52
2022-06-30 21:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:52:39 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.075 | nll_loss 3.596 | ppl 12.09 | wps 0 | wpb 15516 | bsz 1014 | num_updates 576 | best_loss 5.075
2022-06-30 21:52:39 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:52:40 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint52.pt (epoch 52 @ 576 updates, score 5.075) (writing took 1.5392246562987566 seconds)
2022-06-30 21:52:40 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-06-30 21:52:40 | INFO | train | epoch 052 | loss 4.971 | nll_loss 3.61 | ppl 12.21 | wps 1496.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 576 | lr 0.00144007 | gnorm 0.666 | loss_scale 32 | train_wall 67 | wall 0
2022-06-30 21:54:15 | INFO | fairseq.trainer | begin training epoch 53
2022-06-30 21:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 21:57:11 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.032 | nll_loss 3.542 | ppl 11.65 | wps 0 | wpb 15516 | bsz 1014 | num_updates 585 | best_loss 5.032
2022-06-30 21:57:11 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 21:57:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint53.pt (epoch 53 @ 585 updates, score 5.032) (writing took 1.5069100931286812 seconds)
2022-06-30 21:57:13 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-06-30 21:57:13 | INFO | train | epoch 053 | loss 4.93 | nll_loss 3.562 | ppl 11.81 | wps 1575.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 585 | lr 0.00146257 | gnorm 0.675 | loss_scale 32 | train_wall 31 | wall 0
2022-06-30 21:58:16 | INFO | fairseq.trainer | begin training epoch 54
2022-06-30 22:00:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:02:38 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 4.996 | nll_loss 3.492 | ppl 11.25 | wps 0 | wpb 15516 | bsz 1014 | num_updates 594 | best_loss 4.996
2022-06-30 22:02:38 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:02:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint54.pt (epoch 54 @ 594 updates, score 4.996) (writing took 1.6011014990508556 seconds)
2022-06-30 22:02:39 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-06-30 22:02:39 | INFO | train | epoch 054 | loss 4.877 | nll_loss 3.497 | ppl 11.29 | wps 1316 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 594 | lr 0.00148507 | gnorm 0.642 | loss_scale 32 | train_wall 71 | wall 0
2022-06-30 22:04:14 | INFO | fairseq.trainer | begin training epoch 55
2022-06-30 22:05:14 | INFO | train_inner | epoch 055:      6 / 9 loss=5.098, nll_loss=3.763, ppl=13.58, wps=1532.3, ups=0.03, wpb=47592.5, bsz=3216.6, num_updates=600, lr=0.00150007, gnorm=0.676, loss_scale=32, train_wall=410, wall=0
2022-06-30 22:05:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:07:28 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.054 | nll_loss 3.56 | ppl 11.8 | wps 0 | wpb 15516 | bsz 1014 | num_updates 603 | best_loss 4.996
2022-06-30 22:07:28 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:07:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint55.pt (epoch 55 @ 603 updates, score 5.054) (writing took 1.0220196284353733 seconds)
2022-06-30 22:07:29 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-06-30 22:07:29 | INFO | train | epoch 055 | loss 4.842 | nll_loss 3.454 | ppl 10.96 | wps 1480.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 603 | lr 0.00150757 | gnorm 0.725 | loss_scale 32 | train_wall 34 | wall 0
2022-06-30 22:09:02 | INFO | fairseq.trainer | begin training epoch 56
2022-06-30 22:10:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:12:07 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 4.992 | nll_loss 3.479 | ppl 11.15 | wps 0 | wpb 15516 | bsz 1014 | num_updates 612 | best_loss 4.992
2022-06-30 22:12:07 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:12:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint56.pt (epoch 56 @ 612 updates, score 4.992) (writing took 1.5437118597328663 seconds)
2022-06-30 22:12:09 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-06-30 22:12:09 | INFO | train | epoch 056 | loss 4.866 | nll_loss 3.479 | ppl 11.15 | wps 1538 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 612 | lr 0.00153007 | gnorm 1.116 | loss_scale 32 | train_wall 34 | wall 0
2022-06-30 22:13:31 | INFO | fairseq.trainer | begin training epoch 57
2022-06-30 22:15:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:17:36 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 4.973 | nll_loss 3.452 | ppl 10.94 | wps 0 | wpb 15516 | bsz 1014 | num_updates 621 | best_loss 4.973
2022-06-30 22:17:36 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:17:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint57.pt (epoch 57 @ 621 updates, score 4.973) (writing took 1.5792550388723612 seconds)
2022-06-30 22:17:38 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-06-30 22:17:38 | INFO | train | epoch 057 | loss 4.798 | nll_loss 3.397 | ppl 10.54 | wps 1305.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 621 | lr 0.00155257 | gnorm 0.686 | loss_scale 32 | train_wall 62 | wall 0
2022-06-30 22:18:43 | INFO | fairseq.trainer | begin training epoch 58
2022-06-30 22:20:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:22:30 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 4.906 | nll_loss 3.38 | ppl 10.41 | wps 0 | wpb 15516 | bsz 1014 | num_updates 630 | best_loss 4.906
2022-06-30 22:22:31 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:22:32 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint58.pt (epoch 58 @ 630 updates, score 4.906) (writing took 1.6103393323719501 seconds)
2022-06-30 22:22:32 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-06-30 22:22:32 | INFO | train | epoch 058 | loss 4.741 | nll_loss 3.33 | ppl 10.05 | wps 1457.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 630 | lr 0.00157507 | gnorm 0.663 | loss_scale 32 | train_wall 64 | wall 0
2022-06-30 22:23:58 | INFO | fairseq.trainer | begin training epoch 59
2022-06-30 22:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:27:04 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 4.878 | nll_loss 3.342 | ppl 10.14 | wps 0 | wpb 15516 | bsz 1014 | num_updates 639 | best_loss 4.878
2022-06-30 22:27:04 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:27:06 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint59.pt (epoch 59 @ 639 updates, score 4.878) (writing took 1.5939098447561264 seconds)
2022-06-30 22:27:06 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-06-30 22:27:06 | INFO | train | epoch 059 | loss 4.686 | nll_loss 3.267 | ppl 9.63 | wps 1570.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 639 | lr 0.00159757 | gnorm 0.488 | loss_scale 32 | train_wall 37 | wall 0
2022-06-30 22:28:41 | INFO | fairseq.trainer | begin training epoch 60
2022-06-30 22:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:32:41 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 4.851 | nll_loss 3.308 | ppl 9.9 | wps 0 | wpb 15516 | bsz 1014 | num_updates 648 | best_loss 4.851
2022-06-30 22:32:41 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:32:43 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint60.pt (epoch 60 @ 648 updates, score 4.851) (writing took 1.518608320504427 seconds)
2022-06-30 22:32:43 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-06-30 22:32:43 | INFO | train | epoch 060 | loss 4.639 | nll_loss 3.21 | ppl 9.25 | wps 1274.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 648 | lr 0.00162007 | gnorm 0.525 | loss_scale 32 | train_wall 62 | wall 0
2022-06-30 22:34:17 | INFO | fairseq.trainer | begin training epoch 61
2022-06-30 22:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:37:33 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 4.85 | nll_loss 3.302 | ppl 9.86 | wps 0 | wpb 15516 | bsz 1014 | num_updates 657 | best_loss 4.85
2022-06-30 22:37:33 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:37:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint61.pt (epoch 61 @ 657 updates, score 4.85) (writing took 1.581785125657916 seconds)
2022-06-30 22:37:35 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-06-30 22:37:35 | INFO | train | epoch 061 | loss 4.605 | nll_loss 3.169 | ppl 8.99 | wps 1470.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 657 | lr 0.00164257 | gnorm 0.597 | loss_scale 32 | train_wall 30 | wall 0
2022-06-30 22:39:10 | INFO | fairseq.trainer | begin training epoch 62
2022-06-30 22:40:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:42:01 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 4.807 | nll_loss 3.251 | ppl 9.52 | wps 0 | wpb 15516 | bsz 1014 | num_updates 666 | best_loss 4.807
2022-06-30 22:42:01 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:42:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint62.pt (epoch 62 @ 666 updates, score 4.807) (writing took 1.5189473554491997 seconds)
2022-06-30 22:42:03 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-06-30 22:42:03 | INFO | train | epoch 062 | loss 4.595 | nll_loss 3.154 | ppl 8.9 | wps 1604.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 666 | lr 0.00166507 | gnorm 0.771 | loss_scale 32 | train_wall 14 | wall 0
2022-06-30 22:43:19 | INFO | fairseq.trainer | begin training epoch 63
2022-06-30 22:45:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:47:47 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 4.802 | nll_loss 3.238 | ppl 9.44 | wps 0 | wpb 15516 | bsz 1014 | num_updates 675 | best_loss 4.802
2022-06-30 22:47:47 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:47:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint63.pt (epoch 63 @ 675 updates, score 4.802) (writing took 1.5077835656702518 seconds)
2022-06-30 22:47:49 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-06-30 22:47:49 | INFO | train | epoch 063 | loss 4.56 | nll_loss 3.113 | ppl 8.65 | wps 1240.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 675 | lr 0.00168757 | gnorm 0.717 | loss_scale 32 | train_wall 99 | wall 0
2022-06-30 22:49:22 | INFO | fairseq.trainer | begin training epoch 64
2022-06-30 22:50:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:52:33 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 4.774 | nll_loss 3.2 | ppl 9.19 | wps 0 | wpb 15516 | bsz 1014 | num_updates 684 | best_loss 4.774
2022-06-30 22:52:33 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:52:34 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint64.pt (epoch 64 @ 684 updates, score 4.774) (writing took 1.564163200557232 seconds)
2022-06-30 22:52:34 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-06-30 22:52:34 | INFO | train | epoch 064 | loss 4.518 | nll_loss 3.061 | ppl 8.34 | wps 1504.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 684 | lr 0.00171007 | gnorm 0.585 | loss_scale 32 | train_wall 31 | wall 0
2022-06-30 22:54:09 | INFO | fairseq.trainer | begin training epoch 65
2022-06-30 22:55:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 22:57:06 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 4.751 | nll_loss 3.175 | ppl 9.03 | wps 0 | wpb 15516 | bsz 1014 | num_updates 693 | best_loss 4.751
2022-06-30 22:57:06 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 22:57:08 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint65.pt (epoch 65 @ 693 updates, score 4.751) (writing took 1.6108526159077883 seconds)
2022-06-30 22:57:08 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-06-30 22:57:08 | INFO | train | epoch 065 | loss 4.49 | nll_loss 3.026 | ppl 8.14 | wps 1570.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 693 | lr 0.00173257 | gnorm 0.678 | loss_scale 32 | train_wall 15 | wall 0
2022-06-30 22:58:41 | INFO | fairseq.trainer | begin training epoch 66
2022-06-30 23:01:07 | INFO | train_inner | epoch 066:      7 / 9 loss=4.639, nll_loss=3.208, ppl=9.24, wps=1432.5, ups=0.03, wpb=48020.6, bsz=3242.6, num_updates=700, lr=0.00175007, gnorm=0.681, loss_scale=32, train_wall=473, wall=0
2022-06-30 23:01:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:03:08 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 4.757 | nll_loss 3.179 | ppl 9.06 | wps 0 | wpb 15516 | bsz 1014 | num_updates 702 | best_loss 4.751
2022-06-30 23:03:08 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:03:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint66.pt (epoch 66 @ 702 updates, score 4.757) (writing took 1.042354853823781 seconds)
2022-06-30 23:03:09 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-06-30 23:03:09 | INFO | train | epoch 066 | loss 4.462 | nll_loss 2.992 | ppl 7.96 | wps 1187.9 | ups 0.02 | wpb 47723 | bsz 3222.2 | num_updates 702 | lr 0.00175506 | gnorm 0.622 | loss_scale 32 | train_wall 18 | wall 0
2022-06-30 23:04:10 | INFO | fairseq.trainer | begin training epoch 67
2022-06-30 23:06:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:07:59 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 4.731 | nll_loss 3.152 | ppl 8.89 | wps 0 | wpb 15516 | bsz 1014 | num_updates 711 | best_loss 4.731
2022-06-30 23:07:59 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:08:01 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint67.pt (epoch 67 @ 711 updates, score 4.731) (writing took 1.5363735798746347 seconds)
2022-06-30 23:08:01 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-06-30 23:08:01 | INFO | train | epoch 067 | loss 4.427 | nll_loss 2.951 | ppl 7.73 | wps 1472.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 711 | lr 0.00177756 | gnorm 0.619 | loss_scale 32 | train_wall 75 | wall 0
2022-06-30 23:09:01 | INFO | fairseq.trainer | begin training epoch 68
2022-06-30 23:10:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:12:38 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 4.7 | nll_loss 3.105 | ppl 8.6 | wps 0 | wpb 15516 | bsz 1014 | num_updates 720 | best_loss 4.7
2022-06-30 23:12:38 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:12:40 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint68.pt (epoch 68 @ 720 updates, score 4.7) (writing took 1.7094971258193254 seconds)
2022-06-30 23:12:40 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-06-30 23:12:40 | INFO | train | epoch 068 | loss 4.389 | nll_loss 2.907 | ppl 7.5 | wps 1538.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 720 | lr 0.00180006 | gnorm 0.597 | loss_scale 32 | train_wall 69 | wall 0
2022-06-30 23:13:39 | INFO | fairseq.trainer | begin training epoch 69
2022-06-30 23:16:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:18:18 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 4.697 | nll_loss 3.104 | ppl 8.6 | wps 0 | wpb 15516 | bsz 1014 | num_updates 729 | best_loss 4.697
2022-06-30 23:18:18 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:18:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint69.pt (epoch 69 @ 729 updates, score 4.697) (writing took 1.5607522390782833 seconds)
2022-06-30 23:18:20 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-06-30 23:18:20 | INFO | train | epoch 069 | loss 4.372 | nll_loss 2.885 | ppl 7.39 | wps 1264.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 729 | lr 0.00182256 | gnorm 0.711 | loss_scale 32 | train_wall 58 | wall 0
2022-06-30 23:19:56 | INFO | fairseq.trainer | begin training epoch 70
2022-06-30 23:21:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:22:55 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 4.671 | nll_loss 3.068 | ppl 8.39 | wps 0 | wpb 15516 | bsz 1014 | num_updates 738 | best_loss 4.671
2022-06-30 23:22:55 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:22:56 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint70.pt (epoch 70 @ 738 updates, score 4.671) (writing took 1.6900316774845123 seconds)
2022-06-30 23:22:57 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-06-30 23:22:57 | INFO | train | epoch 070 | loss 4.354 | nll_loss 2.863 | ppl 7.27 | wps 1552.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 738 | lr 0.00184506 | gnorm 0.788 | loss_scale 32 | train_wall 25 | wall 0
2022-06-30 23:24:21 | INFO | fairseq.trainer | begin training epoch 71
2022-06-30 23:25:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:27:26 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 4.663 | nll_loss 3.062 | ppl 8.35 | wps 0 | wpb 15516 | bsz 1014 | num_updates 747 | best_loss 4.663
2022-06-30 23:27:26 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:27:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint71.pt (epoch 71 @ 747 updates, score 4.663) (writing took 1.5829436723142862 seconds)
2022-06-30 23:27:27 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-06-30 23:27:27 | INFO | train | epoch 071 | loss 4.317 | nll_loss 2.818 | ppl 7.05 | wps 1585.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 747 | lr 0.00186756 | gnorm 0.619 | loss_scale 32 | train_wall 35 | wall 0
2022-06-30 23:29:03 | INFO | fairseq.trainer | begin training epoch 72
2022-06-30 23:31:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:33:18 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4.655 | nll_loss 3.056 | ppl 8.31 | wps 0 | wpb 15516 | bsz 1014 | num_updates 756 | best_loss 4.655
2022-06-30 23:33:19 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:33:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint72.pt (epoch 72 @ 756 updates, score 4.655) (writing took 1.6600333787500858 seconds)
2022-06-30 23:33:20 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-06-30 23:33:20 | INFO | train | epoch 072 | loss 4.286 | nll_loss 2.78 | ppl 6.87 | wps 1217.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 756 | lr 0.00189006 | gnorm 0.592 | loss_scale 32 | train_wall 11 | wall 0
2022-06-30 23:34:48 | INFO | fairseq.trainer | begin training epoch 73
2022-06-30 23:36:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:37:58 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 4.64 | nll_loss 3.031 | ppl 8.18 | wps 0 | wpb 15516 | bsz 1014 | num_updates 765 | best_loss 4.64
2022-06-30 23:37:58 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:37:59 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint73.pt (epoch 73 @ 765 updates, score 4.64) (writing took 1.5767827183008194 seconds)
2022-06-30 23:38:00 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-06-30 23:38:00 | INFO | train | epoch 073 | loss 4.272 | nll_loss 2.764 | ppl 6.79 | wps 1537.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 765 | lr 0.00191256 | gnorm 0.651 | loss_scale 32 | train_wall 36 | wall 0
2022-06-30 23:39:28 | INFO | fairseq.trainer | begin training epoch 74
2022-06-30 23:40:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:42:28 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.607 | nll_loss 2.992 | ppl 7.96 | wps 0 | wpb 15516 | bsz 1014 | num_updates 774 | best_loss 4.607
2022-06-30 23:42:28 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:42:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint74.pt (epoch 74 @ 774 updates, score 4.607) (writing took 1.6078816335648298 seconds)
2022-06-30 23:42:30 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-06-30 23:42:30 | INFO | train | epoch 074 | loss 4.24 | nll_loss 2.726 | ppl 6.62 | wps 1588.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 774 | lr 0.00193506 | gnorm 0.616 | loss_scale 32 | train_wall 36 | wall 0
2022-06-30 23:44:04 | INFO | fairseq.trainer | begin training epoch 75
2022-06-30 23:45:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:48:02 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.613 | nll_loss 2.992 | ppl 7.96 | wps 0 | wpb 15516 | bsz 1014 | num_updates 783 | best_loss 4.607
2022-06-30 23:48:02 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:48:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint75.pt (epoch 75 @ 783 updates, score 4.613) (writing took 1.0808225963264704 seconds)
2022-06-30 23:48:04 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-06-30 23:48:04 | INFO | train | epoch 075 | loss 4.226 | nll_loss 2.708 | ppl 6.54 | wps 1287.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 783 | lr 0.00195756 | gnorm 0.721 | loss_scale 32 | train_wall 32 | wall 0
2022-06-30 23:49:37 | INFO | fairseq.trainer | begin training epoch 76
2022-06-30 23:50:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:52:44 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4.599 | nll_loss 2.981 | ppl 7.89 | wps 0 | wpb 15516 | bsz 1014 | num_updates 792 | best_loss 4.599
2022-06-30 23:52:44 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:52:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint76.pt (epoch 76 @ 792 updates, score 4.599) (writing took 1.5836317799985409 seconds)
2022-06-30 23:52:46 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-06-30 23:52:46 | INFO | train | epoch 076 | loss 4.209 | nll_loss 2.688 | ppl 6.44 | wps 1520.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 792 | lr 0.00198006 | gnorm 0.658 | loss_scale 32 | train_wall 36 | wall 0
2022-06-30 23:54:15 | INFO | fairseq.trainer | begin training epoch 77
2022-06-30 23:55:27 | INFO | train_inner | epoch 077:      8 / 9 loss=4.3, nll_loss=2.798, ppl=6.95, wps=1461, ups=0.03, wpb=47628.8, bsz=3216.2, num_updates=800, lr=0.00200006, gnorm=0.646, loss_scale=32, train_wall=445, wall=0
2022-06-30 23:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-06-30 23:57:21 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.614 | nll_loss 2.989 | ppl 7.94 | wps 0 | wpb 15516 | bsz 1014 | num_updates 801 | best_loss 4.599
2022-06-30 23:57:21 | INFO | fairseq_cli.train | begin save checkpoint
2022-06-30 23:57:23 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint77.pt (epoch 77 @ 801 updates, score 4.614) (writing took 1.1086221728473902 seconds)
2022-06-30 23:57:23 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-06-30 23:57:23 | INFO | train | epoch 077 | loss 4.167 | nll_loss 2.638 | ppl 6.22 | wps 1552.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 801 | lr 0.00200256 | gnorm 0.571 | loss_scale 32 | train_wall 32 | wall 0
2022-06-30 23:58:48 | INFO | fairseq.trainer | begin training epoch 78
2022-07-01 00:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:02:34 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 4.557 | nll_loss 2.927 | ppl 7.61 | wps 0 | wpb 15516 | bsz 1014 | num_updates 810 | best_loss 4.557
2022-07-01 00:02:34 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:02:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint78.pt (epoch 78 @ 810 updates, score 4.557) (writing took 1.5822995193302631 seconds)
2022-07-01 00:02:35 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-07-01 00:02:35 | INFO | train | epoch 078 | loss 4.165 | nll_loss 2.636 | ppl 6.21 | wps 1373.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 810 | lr 0.00202506 | gnorm 0.786 | loss_scale 32 | train_wall 43 | wall 0
2022-07-01 00:04:08 | INFO | fairseq.trainer | begin training epoch 79
2022-07-01 00:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:07:09 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.538 | nll_loss 2.902 | ppl 7.47 | wps 0 | wpb 15516 | bsz 1014 | num_updates 819 | best_loss 4.538
2022-07-01 00:07:09 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:07:11 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint79.pt (epoch 79 @ 819 updates, score 4.538) (writing took 1.5887657180428505 seconds)
2022-07-01 00:07:11 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-07-01 00:07:11 | INFO | train | epoch 079 | loss 4.116 | nll_loss 2.577 | ppl 5.97 | wps 1558.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 819 | lr 0.00204756 | gnorm 0.543 | loss_scale 32 | train_wall 18 | wall 0
2022-07-01 00:08:38 | INFO | fairseq.trainer | begin training epoch 80
2022-07-01 00:09:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:11:41 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.564 | nll_loss 2.931 | ppl 7.63 | wps 0 | wpb 15516 | bsz 1014 | num_updates 828 | best_loss 4.538
2022-07-01 00:11:41 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:11:42 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint80.pt (epoch 80 @ 828 updates, score 4.564) (writing took 1.063041664659977 seconds)
2022-07-01 00:11:42 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-07-01 00:11:42 | INFO | train | epoch 080 | loss 4.132 | nll_loss 2.596 | ppl 6.05 | wps 1582.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 828 | lr 0.00207006 | gnorm 0.797 | loss_scale 32 | train_wall 21 | wall 0
2022-07-01 00:13:09 | INFO | fairseq.trainer | begin training epoch 81
2022-07-01 00:14:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:17:16 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 4.518 | nll_loss 2.879 | ppl 7.36 | wps 0 | wpb 15516 | bsz 1014 | num_updates 837 | best_loss 4.518
2022-07-01 00:17:17 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:17:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint81.pt (epoch 81 @ 837 updates, score 4.518) (writing took 1.5876435320824385 seconds)
2022-07-01 00:17:18 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-07-01 00:17:18 | INFO | train | epoch 081 | loss 4.093 | nll_loss 2.551 | ppl 5.86 | wps 1278.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 837 | lr 0.00209256 | gnorm 0.545 | loss_scale 32 | train_wall 12 | wall 0
2022-07-01 00:18:49 | INFO | fairseq.trainer | begin training epoch 82
2022-07-01 00:20:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:22:04 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 4.522 | nll_loss 2.876 | ppl 7.34 | wps 0 | wpb 15516 | bsz 1014 | num_updates 846 | best_loss 4.518
2022-07-01 00:22:04 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:22:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint82.pt (epoch 82 @ 846 updates, score 4.522) (writing took 1.0987898726016283 seconds)
2022-07-01 00:22:05 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-07-01 00:22:05 | INFO | train | epoch 082 | loss 4.059 | nll_loss 2.51 | ppl 5.7 | wps 1496.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 846 | lr 0.00211506 | gnorm 0.552 | loss_scale 32 | train_wall 38 | wall 0
2022-07-01 00:23:32 | INFO | fairseq.trainer | begin training epoch 83
2022-07-01 00:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:26:34 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 4.485 | nll_loss 2.844 | ppl 7.18 | wps 0 | wpb 15516 | bsz 1014 | num_updates 855 | best_loss 4.485
2022-07-01 00:26:34 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:26:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint83.pt (epoch 83 @ 855 updates, score 4.485) (writing took 1.6135273650288582 seconds)
2022-07-01 00:26:36 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-07-01 00:26:36 | INFO | train | epoch 083 | loss 4.035 | nll_loss 2.481 | ppl 5.58 | wps 1587.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 855 | lr 0.00213756 | gnorm 0.449 | loss_scale 32 | train_wall 40 | wall 0
2022-07-01 00:28:02 | INFO | fairseq.trainer | begin training epoch 84
2022-07-01 00:29:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:32:01 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 4.549 | nll_loss 2.909 | ppl 7.51 | wps 0 | wpb 15516 | bsz 1014 | num_updates 864 | best_loss 4.485
2022-07-01 00:32:02 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:32:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint84.pt (epoch 84 @ 864 updates, score 4.549) (writing took 1.0717968828976154 seconds)
2022-07-01 00:32:03 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-07-01 00:32:03 | INFO | train | epoch 084 | loss 4.052 | nll_loss 2.5 | ppl 5.66 | wps 1313.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 864 | lr 0.00216006 | gnorm 0.882 | loss_scale 32 | train_wall 30 | wall 0
2022-07-01 00:33:27 | INFO | fairseq.trainer | begin training epoch 85
2022-07-01 00:34:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:36:56 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 4.52 | nll_loss 2.886 | ppl 7.39 | wps 0 | wpb 15516 | bsz 1014 | num_updates 873 | best_loss 4.485
2022-07-01 00:36:56 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:36:57 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint85.pt (epoch 85 @ 873 updates, score 4.52) (writing took 1.1150900702923536 seconds)
2022-07-01 00:36:57 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-07-01 00:36:57 | INFO | train | epoch 085 | loss 4.038 | nll_loss 2.485 | ppl 5.6 | wps 1458.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 873 | lr 0.00218256 | gnorm 0.653 | loss_scale 32 | train_wall 43 | wall 0
2022-07-01 00:38:26 | INFO | fairseq.trainer | begin training epoch 86
2022-07-01 00:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:41:31 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 4.476 | nll_loss 2.836 | ppl 7.14 | wps 0 | wpb 15516 | bsz 1014 | num_updates 882 | best_loss 4.476
2022-07-01 00:41:31 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:41:33 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint86.pt (epoch 86 @ 882 updates, score 4.476) (writing took 1.575291322544217 seconds)
2022-07-01 00:41:33 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-07-01 00:41:33 | INFO | train | epoch 086 | loss 4 | nll_loss 2.439 | ppl 5.42 | wps 1557.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 882 | lr 0.00220506 | gnorm 0.538 | loss_scale 32 | train_wall 36 | wall 0
2022-07-01 00:43:05 | INFO | fairseq.trainer | begin training epoch 87
2022-07-01 00:45:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:47:07 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 4.448 | nll_loss 2.805 | ppl 6.99 | wps 0 | wpb 15516 | bsz 1014 | num_updates 891 | best_loss 4.448
2022-07-01 00:47:07 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:47:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint87.pt (epoch 87 @ 891 updates, score 4.448) (writing took 2.4995766449719667 seconds)
2022-07-01 00:47:10 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-07-01 00:47:10 | INFO | train | epoch 087 | loss 3.985 | nll_loss 2.422 | ppl 5.36 | wps 1274.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 891 | lr 0.00222756 | gnorm 0.629 | loss_scale 32 | train_wall 59 | wall 0
2022-07-01 00:48:46 | INFO | fairseq.trainer | begin training epoch 88
2022-07-01 00:50:10 | INFO | train_inner | epoch 088:      9 / 9 loss=4.058, nll_loss=2.509, ppl=5.69, wps=1446.8, ups=0.03, wpb=47500.5, bsz=3205.7, num_updates=900, lr=0.00225005, gnorm=0.623, loss_scale=32, train_wall=351, wall=0
2022-07-01 00:50:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:51:56 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 4.455 | nll_loss 2.793 | ppl 6.93 | wps 0 | wpb 15516 | bsz 1014 | num_updates 900 | best_loss 4.448
2022-07-01 00:51:56 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:51:57 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint88.pt (epoch 88 @ 900 updates, score 4.455) (writing took 1.0702910386025906 seconds)
2022-07-01 00:51:57 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-07-01 00:51:57 | INFO | train | epoch 088 | loss 3.946 | nll_loss 2.379 | ppl 5.2 | wps 1496.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 900 | lr 0.00225005 | gnorm 0.456 | loss_scale 32 | train_wall 8 | wall 0
2022-07-01 00:53:32 | INFO | fairseq.trainer | begin training epoch 89
2022-07-01 00:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 00:57:03 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 4.448 | nll_loss 2.792 | ppl 6.93 | wps 0 | wpb 15516 | bsz 1014 | num_updates 909 | best_loss 4.448
2022-07-01 00:57:03 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 00:57:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint89.pt (epoch 89 @ 909 updates, score 4.448) (writing took 1.5847490224987268 seconds)
2022-07-01 00:57:04 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-07-01 00:57:04 | INFO | train | epoch 089 | loss 3.919 | nll_loss 2.344 | ppl 5.08 | wps 1398 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 909 | lr 0.00227255 | gnorm 0.466 | loss_scale 32 | train_wall 13 | wall 0
2022-07-01 00:58:33 | INFO | fairseq.trainer | begin training epoch 90
2022-07-01 01:00:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:02:18 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 4.45 | nll_loss 2.796 | ppl 6.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 918 | best_loss 4.448
2022-07-01 01:02:18 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:02:20 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint90.pt (epoch 90 @ 918 updates, score 4.45) (writing took 1.3028412908315659 seconds)
2022-07-01 01:02:20 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-07-01 01:02:20 | INFO | train | epoch 090 | loss 3.904 | nll_loss 2.328 | ppl 5.02 | wps 1362.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 918 | lr 0.00229505 | gnorm 0.556 | loss_scale 32 | train_wall 76 | wall 0
2022-07-01 01:03:54 | INFO | fairseq.trainer | begin training epoch 91
2022-07-01 01:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:07:36 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 4.424 | nll_loss 2.758 | ppl 6.77 | wps 0 | wpb 15516 | bsz 1014 | num_updates 927 | best_loss 4.424
2022-07-01 01:07:37 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:07:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint91.pt (epoch 91 @ 927 updates, score 4.424) (writing took 1.6076855901628733 seconds)
2022-07-01 01:07:38 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-07-01 01:07:38 | INFO | train | epoch 091 | loss 3.879 | nll_loss 2.299 | ppl 4.92 | wps 1347.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 927 | lr 0.00231755 | gnorm 0.45 | loss_scale 32 | train_wall 6 | wall 0
2022-07-01 01:09:03 | INFO | fairseq.trainer | begin training epoch 92
2022-07-01 01:10:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:12:20 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 4.42 | nll_loss 2.759 | ppl 6.77 | wps 0 | wpb 15516 | bsz 1014 | num_updates 936 | best_loss 4.42
2022-07-01 01:12:20 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:12:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint92.pt (epoch 92 @ 936 updates, score 4.42) (writing took 1.6915524695068598 seconds)
2022-07-01 01:12:22 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-07-01 01:12:22 | INFO | train | epoch 092 | loss 3.871 | nll_loss 2.289 | ppl 4.89 | wps 1516 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 936 | lr 0.00234005 | gnorm 0.54 | loss_scale 32 | train_wall 36 | wall 0
2022-07-01 01:13:55 | INFO | fairseq.trainer | begin training epoch 93
2022-07-01 01:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:17:08 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 4.445 | nll_loss 2.805 | ppl 6.99 | wps 0 | wpb 15516 | bsz 1014 | num_updates 945 | best_loss 4.42
2022-07-01 01:17:08 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:17:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint93.pt (epoch 93 @ 945 updates, score 4.445) (writing took 1.103034496307373 seconds)
2022-07-01 01:17:09 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-07-01 01:17:09 | INFO | train | epoch 093 | loss 3.887 | nll_loss 2.307 | ppl 4.95 | wps 1494.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 945 | lr 0.00236255 | gnorm 0.763 | loss_scale 32 | train_wall 3 | wall 0
2022-07-01 01:18:43 | INFO | fairseq.trainer | begin training epoch 94
2022-07-01 01:19:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:21:42 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 4.439 | nll_loss 2.771 | ppl 6.82 | wps 0 | wpb 15516 | bsz 1014 | num_updates 954 | best_loss 4.42
2022-07-01 01:21:42 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:21:43 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint94.pt (epoch 94 @ 954 updates, score 4.439) (writing took 1.0349169950932264 seconds)
2022-07-01 01:21:43 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-07-01 01:21:43 | INFO | train | epoch 094 | loss 3.881 | nll_loss 2.302 | ppl 4.93 | wps 1566.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 954 | lr 0.00238505 | gnorm 0.637 | loss_scale 32 | train_wall 29 | wall 0
2022-07-01 01:23:17 | INFO | fairseq.trainer | begin training epoch 95
2022-07-01 01:25:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:27:25 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 4.396 | nll_loss 2.737 | ppl 6.66 | wps 0 | wpb 15516 | bsz 1014 | num_updates 963 | best_loss 4.396
2022-07-01 01:27:25 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:27:27 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint95.pt (epoch 95 @ 963 updates, score 4.396) (writing took 1.913830554112792 seconds)
2022-07-01 01:27:27 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-07-01 01:27:27 | INFO | train | epoch 095 | loss 3.862 | nll_loss 2.276 | ppl 4.84 | wps 1249.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 963 | lr 0.00240755 | gnorm 0.554 | loss_scale 32 | train_wall 11 | wall 0
2022-07-01 01:28:52 | INFO | fairseq.trainer | begin training epoch 96
2022-07-01 01:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:32:08 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 4.374 | nll_loss 2.71 | ppl 6.54 | wps 0 | wpb 15516 | bsz 1014 | num_updates 972 | best_loss 4.374
2022-07-01 01:32:08 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:32:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint96.pt (epoch 96 @ 972 updates, score 4.374) (writing took 1.5316504929214716 seconds)
2022-07-01 01:32:10 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-07-01 01:32:10 | INFO | train | epoch 096 | loss 3.826 | nll_loss 2.235 | ppl 4.71 | wps 1519.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 972 | lr 0.00243005 | gnorm 0.472 | loss_scale 32 | train_wall 34 | wall 0
2022-07-01 01:33:42 | INFO | fairseq.trainer | begin training epoch 97
2022-07-01 01:34:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:36:43 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 4.415 | nll_loss 2.755 | ppl 6.75 | wps 0 | wpb 15516 | bsz 1014 | num_updates 981 | best_loss 4.374
2022-07-01 01:36:44 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:36:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint97.pt (epoch 97 @ 981 updates, score 4.415) (writing took 1.4028154332190752 seconds)
2022-07-01 01:36:45 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-07-01 01:36:45 | INFO | train | epoch 097 | loss 3.809 | nll_loss 2.217 | ppl 4.65 | wps 1559.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 981 | lr 0.00245255 | gnorm 0.566 | loss_scale 32 | train_wall 34 | wall 0
2022-07-01 01:38:05 | INFO | fairseq.trainer | begin training epoch 98
2022-07-01 01:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:41:55 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 4.366 | nll_loss 2.708 | ppl 6.53 | wps 0 | wpb 15516 | bsz 1014 | num_updates 990 | best_loss 4.366
2022-07-01 01:41:55 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:41:56 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint98.pt (epoch 98 @ 990 updates, score 4.366) (writing took 1.619104167446494 seconds)
2022-07-01 01:41:56 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-07-01 01:41:56 | INFO | train | epoch 098 | loss 3.792 | nll_loss 2.196 | ppl 4.58 | wps 1380 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 990 | lr 0.00247505 | gnorm 0.586 | loss_scale 32 | train_wall 55 | wall 0
2022-07-01 01:43:31 | INFO | fairseq.trainer | begin training epoch 99
2022-07-01 01:45:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:46:56 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 4.389 | nll_loss 2.715 | ppl 6.56 | wps 0 | wpb 15516 | bsz 1014 | num_updates 999 | best_loss 4.366
2022-07-01 01:46:56 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:46:58 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint99.pt (epoch 99 @ 999 updates, score 4.389) (writing took 1.1972294375300407 seconds)
2022-07-01 01:46:58 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-07-01 01:46:58 | INFO | train | epoch 099 | loss 3.783 | nll_loss 2.186 | ppl 4.55 | wps 1425.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 999 | lr 0.00249755 | gnorm 0.528 | loss_scale 32 | train_wall 9 | wall 0
2022-07-01 01:48:31 | INFO | fairseq.trainer | begin training epoch 100
2022-07-01 01:49:11 | INFO | train_inner | epoch 100:      1 / 9 loss=3.852, nll_loss=2.266, ppl=4.81, wps=1349.5, ups=0.03, wpb=47791.3, bsz=3231.2, num_updates=1000, lr=0.00250005, gnorm=0.555, loss_scale=32, train_wall=310, wall=0
2022-07-01 01:49:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:52:03 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 4.388 | nll_loss 2.72 | ppl 6.59 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1008 | best_loss 4.366
2022-07-01 01:52:03 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:52:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint100.pt (epoch 100 @ 1008 updates, score 4.388) (writing took 1.4099454376846552 seconds)
2022-07-01 01:52:04 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-07-01 01:52:04 | INFO | train | epoch 100 | loss 3.776 | nll_loss 2.178 | ppl 4.52 | wps 1400.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1008 | lr 0.00252005 | gnorm 0.666 | loss_scale 32 | train_wall 23 | wall 0
2022-07-01 01:53:45 | INFO | fairseq.trainer | begin training epoch 101
2022-07-01 01:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 01:56:46 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 4.449 | nll_loss 2.784 | ppl 6.89 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1017 | best_loss 4.366
2022-07-01 01:56:46 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 01:56:48 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint101.pt (epoch 101 @ 1017 updates, score 4.449) (writing took 1.2870526388287544 seconds)
2022-07-01 01:56:48 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-07-01 01:56:48 | INFO | train | epoch 101 | loss 3.763 | nll_loss 2.164 | ppl 4.48 | wps 1515.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1017 | lr 0.00254255 | gnorm 0.569 | loss_scale 32 | train_wall 33 | wall 0
2022-07-01 01:58:22 | INFO | fairseq.trainer | begin training epoch 102
2022-07-01 01:59:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:01:28 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 4.336 | nll_loss 2.664 | ppl 6.34 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1026 | best_loss 4.336
2022-07-01 02:01:29 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:01:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint102.pt (epoch 102 @ 1026 updates, score 4.336) (writing took 1.5252585913985968 seconds)
2022-07-01 02:01:30 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-07-01 02:01:30 | INFO | train | epoch 102 | loss 3.762 | nll_loss 2.159 | ppl 4.47 | wps 1522 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1026 | lr 0.00256505 | gnorm 0.62 | loss_scale 32 | train_wall 11 | wall 0
2022-07-01 02:02:53 | INFO | fairseq.trainer | begin training epoch 103
2022-07-01 02:04:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:06:54 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 4.368 | nll_loss 2.701 | ppl 6.5 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1035 | best_loss 4.336
2022-07-01 02:06:54 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:06:56 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint103.pt (epoch 103 @ 1035 updates, score 4.368) (writing took 1.3310205563902855 seconds)
2022-07-01 02:06:56 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-07-01 02:06:56 | INFO | train | epoch 103 | loss 3.74 | nll_loss 2.134 | ppl 4.39 | wps 1318.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1035 | lr 0.00258755 | gnorm 0.648 | loss_scale 32 | train_wall 39 | wall 0
2022-07-01 02:08:22 | INFO | fairseq.trainer | begin training epoch 104
2022-07-01 02:09:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:11:43 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 4.332 | nll_loss 2.665 | ppl 6.34 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1044 | best_loss 4.332
2022-07-01 02:11:44 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:11:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint104.pt (epoch 104 @ 1044 updates, score 4.332) (writing took 2.5306197181344032 seconds)
2022-07-01 02:11:46 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-07-01 02:11:46 | INFO | train | epoch 104 | loss 3.728 | nll_loss 2.123 | ppl 4.36 | wps 1479.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1044 | lr 0.00261005 | gnorm 0.584 | loss_scale 32 | train_wall 47 | wall 0
2022-07-01 02:13:17 | INFO | fairseq.trainer | begin training epoch 105
2022-07-01 02:16:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:17:55 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 4.347 | nll_loss 2.672 | ppl 6.37 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1053 | best_loss 4.332
2022-07-01 02:17:55 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:17:56 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint105.pt (epoch 105 @ 1053 updates, score 4.347) (writing took 1.059018511325121 seconds)
2022-07-01 02:17:56 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-07-01 02:17:56 | INFO | train | epoch 105 | loss 3.698 | nll_loss 2.087 | ppl 4.25 | wps 1160.2 | ups 0.02 | wpb 47723 | bsz 3222.2 | num_updates 1053 | lr 0.00263255 | gnorm 0.501 | loss_scale 32 | train_wall 75 | wall 0
2022-07-01 02:19:23 | INFO | fairseq.trainer | begin training epoch 106
2022-07-01 02:21:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:23:59 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 4.328 | nll_loss 2.655 | ppl 6.3 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1062 | best_loss 4.328
2022-07-01 02:23:59 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:24:01 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint106.pt (epoch 106 @ 1062 updates, score 4.328) (writing took 1.7348705008625984 seconds)
2022-07-01 02:24:01 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-07-01 02:24:01 | INFO | train | epoch 106 | loss 3.674 | nll_loss 2.06 | ppl 4.17 | wps 1178.4 | ups 0.02 | wpb 47723 | bsz 3222.2 | num_updates 1062 | lr 0.00265505 | gnorm 0.43 | loss_scale 32 | train_wall 73 | wall 0
2022-07-01 02:25:20 | INFO | fairseq.trainer | begin training epoch 107
2022-07-01 02:26:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:28:36 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 4.364 | nll_loss 2.693 | ppl 6.47 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1071 | best_loss 4.328
2022-07-01 02:28:36 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:28:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint107.pt (epoch 107 @ 1071 updates, score 4.364) (writing took 1.2341238763183355 seconds)
2022-07-01 02:28:38 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-07-01 02:28:38 | INFO | train | epoch 107 | loss 3.667 | nll_loss 2.052 | ppl 4.15 | wps 1550.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1071 | lr 0.00267755 | gnorm 0.496 | loss_scale 32 | train_wall 51 | wall 0
2022-07-01 02:30:01 | INFO | fairseq.trainer | begin training epoch 108
2022-07-01 02:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:33:27 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 4.325 | nll_loss 2.653 | ppl 6.29 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1080 | best_loss 4.325
2022-07-01 02:33:28 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:33:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint108.pt (epoch 108 @ 1080 updates, score 4.325) (writing took 1.5761985126882792 seconds)
2022-07-01 02:33:29 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-07-01 02:33:29 | INFO | train | epoch 108 | loss 3.669 | nll_loss 2.052 | ppl 4.15 | wps 1473.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1080 | lr 0.00270005 | gnorm 0.596 | loss_scale 32 | train_wall 63 | wall 0
2022-07-01 02:35:04 | INFO | fairseq.trainer | begin training epoch 109
2022-07-01 02:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:38:38 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 4.32 | nll_loss 2.655 | ppl 6.3 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1089 | best_loss 4.32
2022-07-01 02:38:38 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:38:40 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint109.pt (epoch 109 @ 1089 updates, score 4.32) (writing took 1.640618683770299 seconds)
2022-07-01 02:38:40 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-07-01 02:38:40 | INFO | train | epoch 109 | loss 3.655 | nll_loss 2.037 | ppl 4.1 | wps 1382.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1089 | lr 0.00272255 | gnorm 0.6 | loss_scale 32 | train_wall 6 | wall 0
2022-07-01 02:40:14 | INFO | fairseq.trainer | begin training epoch 110
2022-07-01 02:41:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:43:21 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 4.308 | nll_loss 2.64 | ppl 6.23 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1098 | best_loss 4.308
2022-07-01 02:43:21 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:43:23 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint110.pt (epoch 110 @ 1098 updates, score 4.308) (writing took 1.6574116684496403 seconds)
2022-07-01 02:43:23 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-07-01 02:43:23 | INFO | train | epoch 110 | loss 3.664 | nll_loss 2.048 | ppl 4.13 | wps 1518.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1098 | lr 0.00274505 | gnorm 0.669 | loss_scale 32 | train_wall 39 | wall 0
2022-07-01 02:44:56 | INFO | fairseq.trainer | begin training epoch 111
2022-07-01 02:46:14 | INFO | train_inner | epoch 111:      2 / 9 loss=3.709, nll_loss=2.099, ppl=4.28, wps=1393.2, ups=0.03, wpb=47682.9, bsz=3215.9, num_updates=1100, lr=0.00275005, gnorm=0.579, loss_scale=32, train_wall=464, wall=0
2022-07-01 02:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:48:42 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 4.281 | nll_loss 2.612 | ppl 6.11 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1107 | best_loss 4.281
2022-07-01 02:48:42 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:48:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint111.pt (epoch 111 @ 1107 updates, score 4.281) (writing took 1.9847873225808144 seconds)
2022-07-01 02:48:44 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-07-01 02:48:44 | INFO | train | epoch 111 | loss 3.636 | nll_loss 2.014 | ppl 4.04 | wps 1336.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1107 | lr 0.00276754 | gnorm 0.469 | loss_scale 32 | train_wall 32 | wall 0
2022-07-01 02:50:10 | INFO | fairseq.trainer | begin training epoch 112
2022-07-01 02:51:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:53:27 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 4.272 | nll_loss 2.613 | ppl 6.12 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1116 | best_loss 4.272
2022-07-01 02:53:27 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:53:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint112.pt (epoch 112 @ 1116 updates, score 4.272) (writing took 1.5923788752406836 seconds)
2022-07-01 02:53:29 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-07-01 02:53:29 | INFO | train | epoch 112 | loss 3.619 | nll_loss 1.993 | ppl 3.98 | wps 1508.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1116 | lr 0.00279004 | gnorm 0.47 | loss_scale 32 | train_wall 6 | wall 0
2022-07-01 02:55:03 | INFO | fairseq.trainer | begin training epoch 113
2022-07-01 02:56:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 02:58:57 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 4.346 | nll_loss 2.68 | ppl 6.41 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1125 | best_loss 4.272
2022-07-01 02:58:57 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 02:58:58 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint113.pt (epoch 113 @ 1125 updates, score 4.346) (writing took 1.133309744298458 seconds)
2022-07-01 02:58:58 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-07-01 02:58:58 | INFO | train | epoch 113 | loss 3.633 | nll_loss 2.013 | ppl 4.04 | wps 1303 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1125 | lr 0.00281254 | gnorm 0.651 | loss_scale 32 | train_wall 58 | wall 0
2022-07-01 03:00:26 | INFO | fairseq.trainer | begin training epoch 114
2022-07-01 03:01:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:04:16 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 4.307 | nll_loss 2.633 | ppl 6.2 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1134 | best_loss 4.272
2022-07-01 03:04:17 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:04:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint114.pt (epoch 114 @ 1134 updates, score 4.307) (writing took 1.0770358853042126 seconds)
2022-07-01 03:04:18 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-07-01 03:04:18 | INFO | train | epoch 114 | loss 3.614 | nll_loss 1.99 | ppl 3.97 | wps 1345.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1134 | lr 0.00283504 | gnorm 0.591 | loss_scale 32 | train_wall 40 | wall 0
2022-07-01 03:05:48 | INFO | fairseq.trainer | begin training epoch 115
2022-07-01 03:06:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:08:44 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 4.307 | nll_loss 2.635 | ppl 6.21 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1143 | best_loss 4.272
2022-07-01 03:08:44 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:08:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint115.pt (epoch 115 @ 1143 updates, score 4.307) (writing took 1.041528770700097 seconds)
2022-07-01 03:08:45 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-07-01 03:08:45 | INFO | train | epoch 115 | loss 3.581 | nll_loss 1.952 | ppl 3.87 | wps 1607.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1143 | lr 0.00285754 | gnorm 0.406 | loss_scale 32 | train_wall 25 | wall 0
2022-07-01 03:09:58 | INFO | fairseq.trainer | begin training epoch 116
2022-07-01 03:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:13:37 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 4.318 | nll_loss 2.647 | ppl 6.26 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1152 | best_loss 4.272
2022-07-01 03:13:38 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:13:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint116.pt (epoch 116 @ 1152 updates, score 4.318) (writing took 1.3779571000486612 seconds)
2022-07-01 03:13:39 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-07-01 03:13:39 | INFO | train | epoch 116 | loss 3.61 | nll_loss 1.985 | ppl 3.96 | wps 1460.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1152 | lr 0.00288004 | gnorm 0.715 | loss_scale 32 | train_wall 57 | wall 0
2022-07-01 03:15:14 | INFO | fairseq.trainer | begin training epoch 117
2022-07-01 03:16:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:18:44 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 4.285 | nll_loss 2.608 | ppl 6.1 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1161 | best_loss 4.272
2022-07-01 03:18:44 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:18:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint117.pt (epoch 117 @ 1161 updates, score 4.285) (writing took 1.0377078503370285 seconds)
2022-07-01 03:18:45 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-07-01 03:18:45 | INFO | train | epoch 117 | loss 3.575 | nll_loss 1.945 | ppl 3.85 | wps 1403.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1161 | lr 0.00290254 | gnorm 0.487 | loss_scale 32 | train_wall 36 | wall 0
2022-07-01 03:20:19 | INFO | fairseq.trainer | begin training epoch 118
2022-07-01 03:21:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:23:25 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 4.286 | nll_loss 2.622 | ppl 6.16 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1170 | best_loss 4.272
2022-07-01 03:23:25 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:23:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint118.pt (epoch 118 @ 1170 updates, score 4.286) (writing took 1.0740307290107012 seconds)
2022-07-01 03:23:26 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-07-01 03:23:26 | INFO | train | epoch 118 | loss 3.578 | nll_loss 1.948 | ppl 3.86 | wps 1530.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1170 | lr 0.00292504 | gnorm 0.616 | loss_scale 32 | train_wall 9 | wall 0
2022-07-01 03:24:57 | INFO | fairseq.trainer | begin training epoch 119
2022-07-01 03:26:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:28:46 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 4.279 | nll_loss 2.585 | ppl 6 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1179 | best_loss 4.272
2022-07-01 03:28:46 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:28:50 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint119.pt (epoch 119 @ 1179 updates, score 4.279) (writing took 4.125740867108107 seconds)
2022-07-01 03:28:50 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-07-01 03:28:50 | INFO | train | epoch 119 | loss 3.557 | nll_loss 1.926 | ppl 3.8 | wps 1324.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1179 | lr 0.00294754 | gnorm 0.538 | loss_scale 32 | train_wall 44 | wall 0
2022-07-01 03:30:27 | INFO | fairseq.trainer | begin training epoch 120
2022-07-01 03:32:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:33:59 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 4.272 | nll_loss 2.597 | ppl 6.05 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1188 | best_loss 4.272
2022-07-01 03:33:59 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:34:01 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint120.pt (epoch 120 @ 1188 updates, score 4.272) (writing took 1.604060960933566 seconds)
2022-07-01 03:34:01 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-07-01 03:34:01 | INFO | train | epoch 120 | loss 3.53 | nll_loss 1.891 | ppl 3.71 | wps 1382.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1188 | lr 0.00297004 | gnorm 0.469 | loss_scale 32 | train_wall 14 | wall 0
2022-07-01 03:35:12 | INFO | fairseq.trainer | begin training epoch 121
2022-07-01 03:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:38:33 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 4.265 | nll_loss 2.579 | ppl 5.98 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1197 | best_loss 4.265
2022-07-01 03:38:34 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:38:35 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint121.pt (epoch 121 @ 1197 updates, score 4.265) (writing took 1.6785458512604237 seconds)
2022-07-01 03:38:35 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-07-01 03:38:35 | INFO | train | epoch 121 | loss 3.514 | nll_loss 1.875 | ppl 3.67 | wps 1563.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1197 | lr 0.00299254 | gnorm 0.439 | loss_scale 32 | train_wall 56 | wall 0
2022-07-01 03:39:38 | INFO | fairseq.trainer | begin training epoch 122
2022-07-01 03:41:42 | INFO | train_inner | epoch 122:      3 / 9 loss=3.586, nll_loss=1.957, ppl=3.88, wps=1431.9, ups=0.03, wpb=47654.7, bsz=3211.7, num_updates=1200, lr=0.00300004, gnorm=0.536, loss_scale=32, train_wall=444, wall=0
2022-07-01 03:42:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:43:44 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 4.271 | nll_loss 2.609 | ppl 6.1 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1206 | best_loss 4.265
2022-07-01 03:43:45 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:43:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint122.pt (epoch 122 @ 1206 updates, score 4.271) (writing took 1.0629460718482733 seconds)
2022-07-01 03:43:46 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-07-01 03:43:46 | INFO | train | epoch 122 | loss 3.52 | nll_loss 1.881 | ppl 3.68 | wps 1384.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1206 | lr 0.00301504 | gnorm 0.55 | loss_scale 32 | train_wall 94 | wall 0
2022-07-01 03:45:20 | INFO | fairseq.trainer | begin training epoch 123
2022-07-01 03:46:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:48:34 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 4.294 | nll_loss 2.62 | ppl 6.15 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1215 | best_loss 4.265
2022-07-01 03:48:35 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:48:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint123.pt (epoch 123 @ 1215 updates, score 4.294) (writing took 1.07274055108428 seconds)
2022-07-01 03:48:36 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-07-01 03:48:36 | INFO | train | epoch 123 | loss 3.508 | nll_loss 1.868 | ppl 3.65 | wps 1480.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1215 | lr 0.00303754 | gnorm 0.542 | loss_scale 32 | train_wall 51 | wall 0
2022-07-01 03:50:03 | INFO | fairseq.trainer | begin training epoch 124
2022-07-01 03:52:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:54:34 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 4.256 | nll_loss 2.58 | ppl 5.98 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1224 | best_loss 4.256
2022-07-01 03:54:34 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:54:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint124.pt (epoch 124 @ 1224 updates, score 4.256) (writing took 1.58801594004035 seconds)
2022-07-01 03:54:36 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-07-01 03:54:36 | INFO | train | epoch 124 | loss 3.522 | nll_loss 1.884 | ppl 3.69 | wps 1193 | ups 0.02 | wpb 47723 | bsz 3222.2 | num_updates 1224 | lr 0.00306004 | gnorm 0.595 | loss_scale 32 | train_wall 81 | wall 0
2022-07-01 03:56:10 | INFO | fairseq.trainer | begin training epoch 125
2022-07-01 03:57:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 03:59:33 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 4.26 | nll_loss 2.6 | ppl 6.06 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1233 | best_loss 4.256
2022-07-01 03:59:33 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 03:59:34 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint125.pt (epoch 125 @ 1233 updates, score 4.26) (writing took 1.0244405306875706 seconds)
2022-07-01 03:59:34 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-07-01 03:59:34 | INFO | train | epoch 125 | loss 3.497 | nll_loss 1.854 | ppl 3.61 | wps 1438.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1233 | lr 0.00308254 | gnorm 0.496 | loss_scale 32 | train_wall 53 | wall 0
2022-07-01 04:01:09 | INFO | fairseq.trainer | begin training epoch 126
2022-07-01 04:02:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:04:50 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 4.277 | nll_loss 2.6 | ppl 6.06 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1242 | best_loss 4.256
2022-07-01 04:04:50 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:04:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint126.pt (epoch 126 @ 1242 updates, score 4.277) (writing took 1.5160183068364859 seconds)
2022-07-01 04:04:52 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-07-01 04:04:52 | INFO | train | epoch 126 | loss 3.494 | nll_loss 1.853 | ppl 3.61 | wps 1353 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1242 | lr 0.00310504 | gnorm 0.542 | loss_scale 32 | train_wall 13 | wall 0
2022-07-01 04:06:26 | INFO | fairseq.trainer | begin training epoch 127
2022-07-01 04:08:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:10:03 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 4.243 | nll_loss 2.574 | ppl 5.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1251 | best_loss 4.243
2022-07-01 04:10:04 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:10:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint127.pt (epoch 127 @ 1251 updates, score 4.243) (writing took 1.5710627771914005 seconds)
2022-07-01 04:10:05 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-07-01 04:10:05 | INFO | train | epoch 127 | loss 3.465 | nll_loss 1.817 | ppl 3.52 | wps 1370.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1251 | lr 0.00312754 | gnorm 0.434 | loss_scale 32 | train_wall 51 | wall 0
2022-07-01 04:11:40 | INFO | fairseq.trainer | begin training epoch 128
2022-07-01 04:12:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:14:39 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 4.283 | nll_loss 2.612 | ppl 6.11 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1260 | best_loss 4.243
2022-07-01 04:14:40 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:14:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint128.pt (epoch 128 @ 1260 updates, score 4.283) (writing took 1.0248038116842508 seconds)
2022-07-01 04:14:41 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-07-01 04:14:41 | INFO | train | epoch 128 | loss 3.474 | nll_loss 1.83 | ppl 3.56 | wps 1559.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1260 | lr 0.00315004 | gnorm 0.644 | loss_scale 32 | train_wall 32 | wall 0
2022-07-01 04:16:20 | INFO | fairseq.trainer | begin training epoch 129
2022-07-01 04:17:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:19:25 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 4.263 | nll_loss 2.592 | ppl 6.03 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1269 | best_loss 4.243
2022-07-01 04:19:25 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:19:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint129.pt (epoch 129 @ 1269 updates, score 4.263) (writing took 1.1490667592734098 seconds)
2022-07-01 04:19:26 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-07-01 04:19:26 | INFO | train | epoch 129 | loss 3.468 | nll_loss 1.821 | ppl 3.53 | wps 1502.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1269 | lr 0.00317254 | gnorm 0.503 | loss_scale 32 | train_wall 28 | wall 0
2022-07-01 04:21:04 | INFO | fairseq.trainer | begin training epoch 130
2022-07-01 04:23:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:24:49 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 4.242 | nll_loss 2.572 | ppl 5.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1278 | best_loss 4.242
2022-07-01 04:24:49 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:24:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint130.pt (epoch 130 @ 1278 updates, score 4.242) (writing took 1.5991896856576204 seconds)
2022-07-01 04:24:51 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-07-01 04:24:51 | INFO | train | epoch 130 | loss 3.458 | nll_loss 1.811 | ppl 3.51 | wps 1322.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1278 | lr 0.00319504 | gnorm 0.525 | loss_scale 32 | train_wall 23 | wall 0
2022-07-01 04:26:25 | INFO | fairseq.trainer | begin training epoch 131
2022-07-01 04:27:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:29:32 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 4.259 | nll_loss 2.588 | ppl 6.01 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1287 | best_loss 4.242
2022-07-01 04:29:32 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:29:33 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint131.pt (epoch 131 @ 1287 updates, score 4.259) (writing took 1.0889699440449476 seconds)
2022-07-01 04:29:33 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-07-01 04:29:33 | INFO | train | epoch 131 | loss 3.437 | nll_loss 1.786 | ppl 3.45 | wps 1524.3 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1287 | lr 0.00321754 | gnorm 0.485 | loss_scale 32 | train_wall 5 | wall 0
2022-07-01 04:31:08 | INFO | fairseq.trainer | begin training epoch 132
2022-07-01 04:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:34:06 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 4.256 | nll_loss 2.576 | ppl 5.96 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1296 | best_loss 4.242
2022-07-01 04:34:06 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:34:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint132.pt (epoch 132 @ 1296 updates, score 4.256) (writing took 1.0304055865854025 seconds)
2022-07-01 04:34:07 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-07-01 04:34:07 | INFO | train | epoch 132 | loss 3.422 | nll_loss 1.769 | ppl 3.41 | wps 1568.1 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1296 | lr 0.00324004 | gnorm 0.492 | loss_scale 32 | train_wall 20 | wall 0
2022-07-01 04:35:42 | INFO | fairseq.trainer | begin training epoch 133
2022-07-01 04:36:49 | INFO | train_inner | epoch 133:      4 / 9 loss=3.474, nll_loss=1.829, ppl=3.55, wps=1445.7, ups=0.03, wpb=47803.2, bsz=3233.8, num_updates=1300, lr=0.00325004, gnorm=0.529, loss_scale=32, train_wall=387, wall=0
2022-07-01 04:37:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:39:07 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 4.228 | nll_loss 2.555 | ppl 5.87 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1305 | best_loss 4.228
2022-07-01 04:39:07 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:39:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint133.pt (epoch 133 @ 1305 updates, score 4.228) (writing took 1.5608304720371962 seconds)
2022-07-01 04:39:09 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-07-01 04:39:09 | INFO | train | epoch 133 | loss 3.431 | nll_loss 1.779 | ppl 3.43 | wps 1421.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1305 | lr 0.00326253 | gnorm 0.564 | loss_scale 32 | train_wall 42 | wall 0
2022-07-01 04:40:36 | INFO | fairseq.trainer | begin training epoch 134
2022-07-01 04:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:44:17 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 4.25 | nll_loss 2.574 | ppl 5.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1314 | best_loss 4.228
2022-07-01 04:44:17 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:44:19 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint134.pt (epoch 134 @ 1314 updates, score 4.25) (writing took 1.3265351001173258 seconds)
2022-07-01 04:44:19 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-07-01 04:44:19 | INFO | train | epoch 134 | loss 3.417 | nll_loss 1.763 | ppl 3.39 | wps 1386.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1314 | lr 0.00328503 | gnorm 0.513 | loss_scale 32 | train_wall 70 | wall 0
2022-07-01 04:45:45 | INFO | fairseq.trainer | begin training epoch 135
2022-07-01 04:48:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:50:12 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 4.24 | nll_loss 2.572 | ppl 5.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1323 | best_loss 4.228
2022-07-01 04:50:12 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:50:13 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint135.pt (epoch 135 @ 1323 updates, score 4.24) (writing took 1.072474518790841 seconds)
2022-07-01 04:50:13 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-07-01 04:50:13 | INFO | train | epoch 135 | loss 3.399 | nll_loss 1.741 | ppl 3.34 | wps 1212.4 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1323 | lr 0.00330753 | gnorm 0.43 | loss_scale 32 | train_wall 90 | wall 0
2022-07-01 04:51:40 | INFO | fairseq.trainer | begin training epoch 136
2022-07-01 04:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:54:40 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 4.239 | nll_loss 2.571 | ppl 5.94 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1332 | best_loss 4.228
2022-07-01 04:54:40 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:54:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint136.pt (epoch 136 @ 1332 updates, score 4.239) (writing took 1.049369489774108 seconds)
2022-07-01 04:54:41 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-07-01 04:54:41 | INFO | train | epoch 136 | loss 3.391 | nll_loss 1.734 | ppl 3.33 | wps 1600.7 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1332 | lr 0.00333003 | gnorm 0.472 | loss_scale 32 | train_wall 38 | wall 0
2022-07-01 04:56:01 | INFO | fairseq.trainer | begin training epoch 137
2022-07-01 04:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 04:59:44 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 4.24 | nll_loss 2.558 | ppl 5.89 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1341 | best_loss 4.228
2022-07-01 04:59:44 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 04:59:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint137.pt (epoch 137 @ 1341 updates, score 4.24) (writing took 5.113614875823259 seconds)
2022-07-01 04:59:49 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-07-01 04:59:49 | INFO | train | epoch 137 | loss 3.382 | nll_loss 1.724 | ppl 3.3 | wps 1395.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1341 | lr 0.00335253 | gnorm 0.452 | loss_scale 32 | train_wall 46 | wall 0
2022-07-01 05:01:30 | INFO | fairseq.trainer | begin training epoch 138
2022-07-01 05:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 05:04:38 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 4.257 | nll_loss 2.586 | ppl 6.01 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1350 | best_loss 4.228
2022-07-01 05:04:38 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 05:04:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint138.pt (epoch 138 @ 1350 updates, score 4.257) (writing took 1.2239917423576117 seconds)
2022-07-01 05:04:39 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-07-01 05:04:39 | INFO | train | epoch 138 | loss 3.383 | nll_loss 1.725 | ppl 3.3 | wps 1480.6 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1350 | lr 0.00337503 | gnorm 0.51 | loss_scale 32 | train_wall 19 | wall 0
2022-07-01 05:06:10 | INFO | fairseq.trainer | begin training epoch 139
2022-07-01 05:07:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 05:09:54 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 4.236 | nll_loss 2.574 | ppl 5.95 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1359 | best_loss 4.228
2022-07-01 05:09:54 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 05:09:56 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint139.pt (epoch 139 @ 1359 updates, score 4.236) (writing took 1.407644521445036 seconds)
2022-07-01 05:09:56 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-07-01 05:09:56 | INFO | train | epoch 139 | loss 3.367 | nll_loss 1.705 | ppl 3.26 | wps 1356.8 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1359 | lr 0.00339753 | gnorm 0.39 | loss_scale 32 | train_wall 40 | wall 0
2022-07-01 05:11:32 | INFO | fairseq.trainer | begin training epoch 140
2022-07-01 05:12:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 05:14:48 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 4.261 | nll_loss 2.598 | ppl 6.06 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1368 | best_loss 4.228
2022-07-01 05:14:48 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 05:14:49 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint140.pt (epoch 140 @ 1368 updates, score 4.261) (writing took 1.0493119433522224 seconds)
2022-07-01 05:14:49 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-07-01 05:14:49 | INFO | train | epoch 140 | loss 3.374 | nll_loss 1.714 | ppl 3.28 | wps 1463.5 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1368 | lr 0.00342003 | gnorm 0.583 | loss_scale 32 | train_wall 4 | wall 0
2022-07-01 05:16:24 | INFO | fairseq.trainer | begin training epoch 141
2022-07-01 05:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 05:19:45 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 4.249 | nll_loss 2.58 | ppl 5.98 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1377 | best_loss 4.228
2022-07-01 05:19:45 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 05:19:47 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint141.pt (epoch 141 @ 1377 updates, score 4.249) (writing took 1.0363561604171991 seconds)
2022-07-01 05:19:47 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-07-01 05:19:47 | INFO | train | epoch 141 | loss 3.372 | nll_loss 1.712 | ppl 3.28 | wps 1443.9 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1377 | lr 0.00344253 | gnorm 0.559 | loss_scale 32 | train_wall 27 | wall 0
2022-07-01 05:21:06 | INFO | fairseq.trainer | begin training epoch 142
2022-07-01 05:23:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 05:24:58 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 4.251 | nll_loss 2.579 | ppl 5.98 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1386 | best_loss 4.228
2022-07-01 05:24:58 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 05:24:59 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint142.pt (epoch 142 @ 1386 updates, score 4.251) (writing took 1.0664714090526104 seconds)
2022-07-01 05:24:59 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-07-01 05:24:59 | INFO | train | epoch 142 | loss 3.371 | nll_loss 1.713 | ppl 3.28 | wps 1374.2 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1386 | lr 0.00346503 | gnorm 0.477 | loss_scale 32 | train_wall 73 | wall 0
2022-07-01 05:26:34 | INFO | fairseq.trainer | begin training epoch 143
2022-07-01 05:28:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-07-01 05:30:20 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 4.234 | nll_loss 2.561 | ppl 5.9 | wps 0 | wpb 15516 | bsz 1014 | num_updates 1395 | best_loss 4.228
2022-07-01 05:30:20 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 10 runs
2022-07-01 05:30:20 | INFO | fairseq_cli.train | begin save checkpoint
2022-07-01 05:30:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0/checkpoint143.pt (epoch 143 @ 1395 updates, score 4.234) (writing took 1.0629123505204916 seconds)
2022-07-01 05:30:22 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-07-01 05:30:22 | INFO | train | epoch 143 | loss 3.349 | nll_loss 1.686 | ppl 3.22 | wps 1332 | ups 0.03 | wpb 47723 | bsz 3222.2 | num_updates 1395 | lr 0.00348753 | gnorm 0.467 | loss_scale 32 | train_wall 24 | wall 0
2022-07-01 05:30:22 | INFO | fairseq_cli.train | done training in 42382.3 seconds
/home/yihang/software/python3/usr/local/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 2256 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
