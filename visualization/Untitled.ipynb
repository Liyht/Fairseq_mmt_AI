{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81357d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2321764238.jpg\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/visualization/images/2321764238.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a189d7409931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mmodel_path\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m'../checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mselective_attention_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a189d7409931>\u001b[0m in \u001b[0;36mselective_attention_visualization\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"images\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/berry/home/yihang/pyenv/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/berry/home/yihang/code/mmt_model_rebuild/fairseq_mmt/visualization/images/2321764238.jpg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###\n",
    "# To visualize attention maps, we need files as follows:\n",
    "# Images(get from multi30k, resized to 224x224):    \t'./images/*.jpg'\n",
    "# Attention maps(saved while translation):     \t \t    './checkpoint/*/visualization/*map.pth'\n",
    "# Src_tokens(saved while translation):        \t \t    './checkpoint/*/visualization/*tokens.pth'\n",
    "# Origin_src_tokens(saved while translating mask0): \t'./origin_tokens/*tokens.pth'\n",
    "# Translation results(saved while translating):  \t    './checkpoint/*/hypo.txt'\n",
    "# Dictionary of src(saved while bpe):    \t \t        './dict.en.txt'\n",
    "# Image filename(get from multi30k):       \t \t        './test_images.txt'\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "def selective_attention_visualization(model_path):\n",
    "    root_path = os.getcwd()\n",
    "    \n",
    "\n",
    "    # Get the translation order from 'hypo.txt'\n",
    "    translation_order_list = []\n",
    "    with open(os.path.join(model_path, 'hypo.txt'), 'r', encoding='utf-8') as translation_order_file:\n",
    "        for line in translation_order_file:\n",
    "            translation_order_list.append(int(line.strip().split('\\t')[0]))\n",
    "\n",
    "    # Get image name list from 'test_images.txt'\n",
    "    test_images_filename_list = []\n",
    "    with open(os.path.join(root_path, 'test_images.txt'), 'r', encoding='utf-8') as test_images_filename_file:\n",
    "        for line in test_images_filename_file:\n",
    "            test_images_filename_list.append(line.strip())\n",
    "\n",
    "    # Get the dictionary{id: word} from 'dict.en.txt'\n",
    "    dic_no2word = {0: '<bos>', 1: '<pad>', 2: '<eos>', 3: '<unk>'}\n",
    "    with open(os.path.join(root_path, 'dict.en.txt'), 'r', encoding='utf-8') as dict_file:\n",
    "        for idx, line in enumerate(dict_file):\n",
    "            dic_no2word[idx+4]  = line.strip().split()[0]\n",
    "\n",
    "    # Attention map and src_tokens are divided into 8 batches with batch_size=128 in translation\n",
    "    for batch in range(8):\n",
    "        # Get attention maps, src_tokens and origin_tokens\n",
    "        attn_map_path = os.path.join(model_path, 'visualization', str(batch) + 'map.pth')\n",
    "        src_tokens_path = os.path.join(model_path, 'visualization', str(batch) + 'tokens.pth')\n",
    "        origin_tokens_path = os.path.join(root_path, 'origin_tokens', str(batch) + 'tokens.pth')\n",
    "\n",
    "        attn_map = torch.load(attn_map_path, map_location=torch.device('cpu'))\n",
    "        src_tokens = torch.load(src_tokens_path, map_location=torch.device('cpu'))\n",
    "        origin_tokens = torch.load(origin_tokens_path, map_location=torch.device('cpu'))\n",
    "\n",
    "        for sent_num in range(attn_map.shape[0]):\n",
    "            filename = test_images_filename_list[translation_order_list[batch * 128 + sent_num]]\n",
    "            # Images for test\n",
    "            if filename != '2321764238.jpg':\n",
    "                continue\n",
    "            # if filename != '327955368.jpg':\n",
    "            #     continue\n",
    "            print(filename)\n",
    "\n",
    "            img = Image.open(os.path.join(root_path, \"images\", filename), mode='r')\n",
    "            plt.figure(filename, figsize=(8, 8))\n",
    "\n",
    "            for word_num in range(attn_map.shape[1]):\n",
    "                # Get the attention map for the word\n",
    "                attn = attn_map[sent_num][word_num].view(24, 24).cpu().numpy()\n",
    "                # Get the word with the dictionary and src_tokens\n",
    "                word = src_tokens.cpu().numpy()[sent_num][word_num]\n",
    "                word = dic_no2word[word]\n",
    "                origin_word = origin_tokens.cpu().numpy()[sent_num][word_num]\n",
    "                origin_word = dic_no2word[origin_word]\n",
    "\n",
    "                # Skip '<pad>' and '<eos>'\n",
    "                if word == '<pad>' or word == '<eos>':\n",
    "                    continue\n",
    "\n",
    "                # Show the image\n",
    "                plt.subplot(math.ceil(attn_map.shape[1] / 4), 4, word_num + 1)\n",
    "                plt.title(word + '-' + origin_word, fontsize=9)\n",
    "                plt.imshow(img, alpha=1)\n",
    "                plt.axis('off')\n",
    "\n",
    "                img_h, img_w = img.size[0], img.size[1]\n",
    "                attn = cv2.resize(attn.astype(np.float32), (img_h, img_w))\n",
    "                normed_attn = attn / attn.max()\n",
    "                normed_attn = (normed_attn * 255).astype('uint8')\n",
    "\n",
    "                # Show the visual attention map of the word\n",
    "                plt.imshow(normed_attn, alpha=0.4, interpolation='nearest', cmap='jet')\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path  = '../checkpoints/multi30k-en2de/vit_base_patch16_384/vit_base_patch16_384-mask0'\n",
    "    selective_attention_visualization(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
